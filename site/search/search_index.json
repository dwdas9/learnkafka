{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#apache-kafka-for-builders","title":"\ud83d\udcd8 Apache Kafka for Builders","text":""},{"location":"#architect-develop-deploy-real-time-apps","title":"Architect, Develop &amp; Deploy Real-Time Apps","text":"<p>Welcome to the most practical, hands-on guide to Apache Kafka. This course is designed for builders \u2014 developers, architects, and engineers who want to learn by doing.</p>"},{"location":"#what-makes-this-different","title":"\ud83c\udfaf What Makes This Different?","text":"<p>Learning Philosophy</p> <ul> <li>Build first, theory second \u2014 Every concept is tied to a real project</li> <li>No fluff \u2014 Only what you'll actually use in production</li> <li>Real architectures \u2014 Learn from actual production patterns</li> <li>Code-heavy \u2014 Docker setups, working examples, mini-projects</li> </ul>"},{"location":"#course-structure","title":"\ud83d\udcda Course Structure","text":"<ul> <li> <p> Part I: Start Fast</p> <p>Get productive with Kafka in hours, not weeks.</p> <p> Begin Learning</p> </li> <li> <p> Part II: Build Applications</p> <p>Design events, manage schemas, and process streams.</p> <p> Build Apps</p> </li> <li> <p>:material-architecture:{ .lg .middle } Part III: Architecture</p> <p>Patterns, anti-patterns, and real-world system designs.</p> <p> Design Systems</p> </li> <li> <p> Part IV: Production</p> <p>Deploy, monitor, secure, and scale Kafka systems.</p> <p> Go Production</p> </li> <li> <p> Part V: Real Projects</p> <p>5 complete projects from order tracking to fraud detection.</p> <p> Build Projects</p> </li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to Kafka? Start here:</p> <ol> <li>Why Kafka? - Understand the problem it solves</li> <li>Core Concepts - Topics, producers, consumers</li> <li>10-Minute Setup - Get Kafka running with Docker</li> </ol> <p>Already know the basics? Jump to:</p> <ul> <li>Stream Processing for real-time analytics</li> <li>Design Patterns for architecture</li> <li>Production Guide for deployment</li> </ul>"},{"location":"#who-this-is-for","title":"\ud83d\udca1 Who This Is For","text":"Backend DevelopersData EngineersArchitectsDevOps Engineers <p>Build event-driven microservices and streaming pipelines</p> <p>Design real-time ETL and data integration workflows</p> <p>Learn scalable patterns for distributed systems</p> <p>Deploy and manage Kafka clusters in production</p>"},{"location":"#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":"<p>Throughout this course, you'll build 5 production-grade projects:</p> Project Skills Learned \ud83d\uded2 Real-time Order Tracking Producers, Consumers, Topics \ud83d\udce6 Inventory Sync System Event-driven Microservices \ud83d\udd0d Fraud Detection Pipeline Kafka Streams, Windowing \ud83d\uddc4\ufe0f Data Warehouse ETL Kafka Connect, Integrations \ud83d\udcca Clickstream Analytics End-to-end Real-time Pipeline"},{"location":"#how-to-use-this-guide","title":"\ud83d\udcd6 How to Use This Guide","text":"<p>Recommended Path</p> <p>For Beginners: Follow the course sequentially from Part I \u2192 Part V</p> <p>For Experienced Users: Jump to relevant sections based on your needs</p> <p>For Hands-on Learners: Go directly to Part V projects and refer back as needed</p>"},{"location":"#prerequisites","title":"\ud83e\udd1d Prerequisites","text":"<ul> <li>Basic programming knowledge (Java or Python preferred)</li> <li>Understanding of REST APIs and databases</li> <li>Docker installed (for hands-on labs)</li> <li>Terminal/command-line familiarity</li> </ul> <p>No prior Kafka knowledge required!</p>"},{"location":"#learning-outcomes","title":"\ud83c\udf93 Learning Outcomes","text":"<p>By the end of this course, you will:</p> <ul> <li> Design and implement event-driven architectures</li> <li> Build producers, consumers, and stream processing apps</li> <li> Deploy and manage Kafka in production environments</li> <li> Troubleshoot common Kafka issues</li> <li> Apply best practices and avoid anti-patterns</li> <li> Integrate Kafka with databases, data lakes, and other systems</li> </ul> <p>Ready to Begin?</p> <p>Start your Kafka journey with Part I: Why Kafka? \u2192</p>"},{"location":"#stay-connected","title":"\ud83d\udcec Stay Connected","text":"<p>Found this helpful? Star the repo on GitHub and share with your team!</p>"},{"location":"pages/Introduction/","title":"Apache Kafka: What is it?","text":""},{"location":"pages/Introduction/#apache-kafka-what-is-it","title":"Apache Kafka: What is it?","text":"<p>You've heard the name. Kafka powers Netflix, Uber, LinkedIn, basically every tech company handling serious data. </p> <p>So what is it? To make it simple, kafka is log-storage. Just like we have SQL databse to store tables.</p> <p>But, its not just a simple log storage. Imagine  LinkedIn processes over a trillion events daily. Clicks, messages, updates, connections. That's not millions. That's trillions.</p> <p>How do you create a system which will store so much of logs, in correct order and let consumers consume it?</p> <p>Before Kafka, companies had two terrible options:</p> <p>Option 1: Wire systems together directly. System A talks to System B, B talks to C, C talks to D. Works fine until you have 20 systems and a tangled mess of connections. Adding one new service means rewiring everything.</p> <p>Option 2: Build custom messaging systems. Every team creates their own queue. Now instead of one mess, you have ten incompatible messes.</p> <p>Neither scales. Neither works.</p> <p>Kafka solved this by becoming the single source of truth for data movement. One system. Built to handle massive scale.</p>"},{"location":"pages/Introduction/#what-kafka-actually-is","title":"What Kafka Actually Is","text":"<p>The official definition: \"A distributed event streaming platform.\"</p> <p>The real definition: Kafka is a append-only log that stores data streams.</p> <p>Think of it like this: - Producers write data into Kafka - Kafka stores that data durably  - Consumers read the data whenever they want</p> <p>The key difference from a regular message queue? Kafka doesn't delete messages after they're read. It keeps them. You can read the same message ten times, or come back tomorrow and replay everything from the beginning.</p> <p>It's more like a database log than a postal service.</p>"},{"location":"pages/Introduction/#the-core-concepts","title":"The Core Concepts","text":"<p>Producer - Writes data to Kafka Consumer - Reads data from Kafka Topic - A category or feed name (like \"user-clicks\" or \"payment-events\") Partition - Topics split into partitions for parallel processing Broker - A Kafka server (you run multiple for redundancy) Consumer Group - Multiple consumers sharing the work of reading a topic Offset - The position of a message in a partition Replication - Copies of data across brokers so nothing gets lost  </p> <p>That's it. Master these and you understand Kafka.</p>"},{"location":"pages/Introduction/#what-is-a-producer","title":"What Is a Producer?","text":"<p>A Producer is not the data source.</p> <ul> <li>Data Sources (like Twitter, Coinbase, or a weather API) merely generate the raw data.</li> <li>The Kafka Producer is the code you write (your script or application) that acts as the bridge. Its only job is to fetch the data from the source, process it, and write it directly into your Kafka topic.</li> </ul> <p>Simply put: The Producer is the program you write with access to your Kafka system, not the external website.</p> <p></p> <p>Let's look at some examples of Kafka Producer Scripts:</p> <p>Example 1: Python script tracking crypto prices</p> <pre><code>from kafka import KafkaProducer\nimport json\nimport requests\nimport time\n\n# Create a producer\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Fetch Bitcoin price every second and send to Kafka\nwhile True:\n    price = requests.get('https://api.coinbase.com/v2/prices/BTC-USD/spot').json()\n\n    # Send to Kafka topic \"crypto-prices\"\n    producer.send('crypto-prices', {\n        'coin': 'BTC',\n        'price': price['data']['amount'],\n        'timestamp': time.time()\n    })\n\n    time.sleep(1)\n</code></pre> <p>That's a producer. It grabs live Bitcoin prices and pushes them into Kafka.</p> <p>Example 2: Web server logging user clicks</p> <pre><code>from kafka import KafkaProducer\nfrom flask import Flask, request\n\napp = Flask(__name__)\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n\n@app.route('/api/click')\ndef track_click():\n    # User clicked something, send event to Kafka\n    producer.send('user-clicks', \n        value=json.dumps({\n            'user_id': request.args.get('user_id'),\n            'page': request.args.get('page'),\n            'timestamp': time.time()\n        }).encode('utf-8')\n    )\n    return \"tracked\"\n</code></pre> <p>Your Flask app is the producer. Every time someone clicks, you write to Kafka.</p> <p>Example 3: IoT sensor data</p> <pre><code>from kafka import KafkaProducer\nimport random\n\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n\n# Temperature sensor sending readings\nwhile True:\n    temperature = random.uniform(20, 30)\n    producer.send('sensor-data', f\"temp:{temperature}\".encode('utf-8'))\n    time.sleep(5)\n</code></pre> <p>The sensor script is the producer.</p> <p>The Pattern</p> <p>Every producer follows the same flow: 1. Connect to Kafka (the brokers) 2. Call <code>producer.send(topic_name, data)</code> 3. Kafka handles the rest</p> <p>The producer doesn't care who reads the data. It just writes and moves on.</p>"},{"location":"pages/Introduction/#what-is-a-consumer-real-code","title":"What Is a Consumer? (Real Code)","text":"<p>A consumer reads data from Kafka. Here's what it looks like:</p> <pre><code>from kafka import KafkaConsumer\nimport json\n\n# Create a consumer\nconsumer = KafkaConsumer(\n    'crypto-prices',  # Topic name\n    bootstrap_servers=['localhost:9092'],\n    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n    group_id='price-alerts'  # Consumer group\n)\n\n# Read messages forever\nfor message in consumer:\n    data = message.value\n    print(f\"BTC price: ${data['price']}\")\n\n    # Do something with it\n    if float(data['price']) &gt; 100000:\n        send_alert(\"Bitcoin over 100k!\")\n</code></pre> <p>That's a consumer. It reads from the topic \"crypto-prices\" and processes each message.</p> <p>Multiple consumers can read the same data:</p> <pre><code># Consumer 1: Price alerts\nconsumer1 = KafkaConsumer('crypto-prices', group_id='alerts')\n\n# Consumer 2: Store to database\nconsumer2 = KafkaConsumer('crypto-prices', group_id='database-writer')\n\n# Consumer 3: Analytics\nconsumer3 = KafkaConsumer('crypto-prices', group_id='analytics')\n</code></pre> <p>All three read the same Bitcoin prices. They each track their own position (offset) independently.</p>"},{"location":"pages/Introduction/#what-is-a-topic-concrete-view","title":"What Is a Topic? (Concrete View)","text":"<p>A topic is just a name for a category of messages. You create it and send data to it.</p> <pre><code># Create a topic\nkafka-topics --create --topic user-events --bootstrap-server localhost:9092\n\n# List all topics\nkafka-topics --list --bootstrap-server localhost:9092\n</code></pre> <p>Output: </p><pre><code>crypto-prices\nuser-clicks\norder-events\nsensor-data\n</code></pre><p></p> <p>Topics are like database tables, but for streams. You write to \"crypto-prices\", you read from \"crypto-prices\".</p>"},{"location":"pages/Introduction/#what-is-a-partition-how-kafka-scales","title":"What Is a Partition? (How Kafka Scales)","text":"<p>Here's where it gets real. A topic splits into partitions for parallel processing.</p> <p>When you create a topic: </p><pre><code>kafka-topics --create --topic crypto-prices --partitions 3 --replication-factor 2\n</code></pre><p></p> <p>Now \"crypto-prices\" has 3 partitions: - crypto-prices-0 - crypto-prices-1 - crypto-prices-2</p> <p>What does this mean?</p> <p>When your producer sends a message, Kafka picks a partition (usually by hashing the key):</p> <pre><code># Messages with same key go to same partition\nproducer.send('crypto-prices', key=b'BTC', value=price_data)\nproducer.send('crypto-prices', key=b'ETH', value=price_data)\n</code></pre> <p>All BTC messages \u2192 partition 0 All ETH messages \u2192 partition 1 All SOL messages \u2192 partition 2</p> <p>Why partitions matter:</p> <p>Each partition can be read by one consumer in a group. More partitions = more parallelism.</p> <pre><code># Consumer group with 3 members\nconsumer1 = KafkaConsumer('crypto-prices', group_id='workers')  # reads partition 0\nconsumer2 = KafkaConsumer('crypto-prices', group_id='workers')  # reads partition 1\nconsumer3 = KafkaConsumer('crypto-prices', group_id='workers')  # reads partition 2\n</code></pre> <p>Three consumers split the work. That's how Kafka scales to trillions of messages.</p>"},{"location":"pages/Introduction/#what-is-an-offset-your-position-in-the-stream","title":"What Is an Offset? (Your Position in the Stream)","text":"<p>Each message in a partition gets a number. That's the offset.</p> <pre><code>Partition 0:\n[0] BTC: $95,000\n[1] BTC: $95,100\n[2] BTC: $95,050\n[3] BTC: $95,200\n      \u2191\n   Your consumer is here (offset 3)\n</code></pre> <p>Kafka tracks where each consumer is. If your consumer crashes and restarts, it picks up at offset 3.</p> <p>You can also manually set offsets:</p> <pre><code># Start from the beginning\nconsumer = KafkaConsumer('crypto-prices', auto_offset_reset='earliest')\n\n# Start from the end (only new messages)\nconsumer = KafkaConsumer('crypto-prices', auto_offset_reset='latest')\n\n# Jump to a specific offset\nconsumer.seek(partition, 100)  # Start at message 100\n</code></pre>"},{"location":"pages/Introduction/#what-is-a-consumer-group-load-balancing","title":"What Is a Consumer Group? (Load Balancing)","text":"<p>A consumer group is multiple consumers sharing the work.</p> <p>Scenario: Processing 1 million transactions/second</p> <pre><code># One consumer - too slow\nconsumer = KafkaConsumer('transactions', group_id='processors')\n\n# Ten consumers in same group - 10x faster\nfor i in range(10):\n    consumer = KafkaConsumer('transactions', group_id='processors')\n    # Each consumer gets different partitions\n</code></pre> <p>Kafka automatically distributes partitions across consumers in the same group. Add more consumers, process faster.</p> <p>Key rule: Max consumers per group = number of partitions.</p> <p>If you have 3 partitions and 5 consumers in a group, 2 consumers sit idle.</p>"},{"location":"pages/Introduction/#how-it-all-connects","title":"How It All Connects","text":"<pre><code># Producer sends 1000 messages/sec\nproducer.send('orders', order_data)\n\n# Topic \"orders\" has 10 partitions\n# Messages spread across partitions 0-9\n\n# Consumer group \"order-processors\" has 10 consumers\n# Each consumer reads 1 partition = 100 messages/sec each\n\n# Consumer group \"analytics\" also reads \"orders\"\n# Independent offset tracking, doesn't affect processors\n</code></pre> <p>That's Kafka. Producers write, topics organize, partitions scale, consumers read, offsets track progress.</p>"},{"location":"pages/Introduction/#how-it-works","title":"How It Works","text":"<p>Say you're building a ride-sharing app.</p> <p>When a user requests a ride, that event goes into Kafka under the topic \"ride-requests\". Kafka writes it to disk across multiple brokers.</p> <p>Now three different systems can consume that same event: - The matching service pairs riders with drivers - The analytics team tracks demand patterns - The billing system prepares to charge the ride</p> <p>Each consumer reads independently. They track their own offset. If analytics goes down for maintenance, it just catches up later by reading from where it left off.</p> <p>Add a fourth consumer tomorrow? Just subscribe to the topic. No rewiring. No breaking existing systems.</p>"},{"location":"pages/Introduction/#why-this-architecture-wins","title":"Why This Architecture Wins","text":"<p>Decoupling - Systems don't know about each other. They only know Kafka. Add new services without touching old ones.</p> <p>Replay - Made a mistake? Reprocess the last hour of data. Kafka still has it.</p> <p>Scale - Add more brokers, add more partitions. Kafka scales horizontally forever.</p> <p>Durability - Data replicates across brokers. Hardware fails? Kafka doesn't care.</p> <p>Speed - Kafka handles millions of messages per second. It's fast enough.</p>"},{"location":"pages/Introduction/#why-everyone-uses-it","title":"Why Everyone Uses It","text":"<p>Before Kafka, building data pipelines was nightmare fuel. Every integration was custom. Every new system meant weeks of work.</p> <p>Kafka changed the game. Write to Kafka once, read from it everywhere. Your stream processing, your databases, your analytics, your ML models - they all plug into the same pipe.</p> <p>Data flows became simple. That's why Kafka won.</p> <p>Next up: How topics and partitions actually work under the hood.</p>"},{"location":"pages/example/","title":"This is a sample page for the MkDocs documentation.","text":""},{"location":"pages/example/#this-is-a-sample-page-for-the-mkdocs-documentation","title":"This is a sample page for the MkDocs documentation.","text":""},{"location":"pages/example/#example-page","title":"Example Page","text":"<p>Welcome to the example page of the MkDocs site. This page can be used to provide additional content or examples relevant to the project.</p>"},{"location":"pages/example/#section-1-introduction","title":"Section 1: Introduction","text":"<p>This section can introduce the topic or provide context for the examples that follow.</p>"},{"location":"pages/example/#section-2-example-content","title":"Section 2: Example Content","text":"<p>Here you can include code snippets, images, or any other relevant information that helps illustrate your points.</p>"},{"location":"pages/example/#code-example","title":"Code Example","text":"<pre><code>def hello_world():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"pages/example/#conclusion","title":"Conclusion","text":"<p>Feel free to modify this page to suit your documentation needs.</p>"},{"location":"part-1/","title":"Part I: Start Fast &amp; Build Something","text":""},{"location":"part-1/#part-i-start-fast-build-something","title":"Part I: Start Fast &amp; Build Something","text":""},{"location":"part-1/#get-productive-with-kafka-quickly","title":"\ud83d\ude80 Get Productive with Kafka Quickly","text":"<p>This section is designed to get you from zero to building in the shortest time possible. No theory overload \u2014 just the essentials you need to start creating real Kafka applications.</p>"},{"location":"part-1/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<p>By the end of Part I, you will:</p> <ul> <li>\u2705 Understand why Kafka exists and when to use it</li> <li>\u2705 Master the core concepts (topics, partitions, producers, consumers)</li> <li>\u2705 Have Kafka running locally via Docker</li> <li>\u2705 Build your first producer and consumer in code</li> <li>\u2705 Complete 2 hands-on mini-projects</li> </ul>"},{"location":"part-1/#chapters","title":"\ud83d\uddfa\ufe0f Chapters","text":"<ul> <li> <p> Chapter 1: Why Kafka?</p> <p>Real-world problems that Kafka solves, use cases, and when NOT to use it.</p> <p> Learn Why</p> </li> <li> <p> Chapter 2: Core Concepts</p> <p>Topics, partitions, offsets, brokers \u2014 explained visually and simply.</p> <p> Master Concepts</p> </li> <li> <p> Chapter 3: 10-Minute Setup</p> <p>Get Kafka running with Docker and send your first message.</p> <p> Quick Setup</p> </li> <li> <p> Chapter 4: Programming Kafka</p> <p>Write producers and consumers in Java/Python with real examples.</p> <p> Start Coding</p> </li> </ul>"},{"location":"part-1/#hands-on-projects","title":"\ud83c\udfaf Hands-On Projects","text":"<p>Each chapter includes practical exercises, culminating in these mini-projects:</p> <p>Mini Project #1: Simple Event Pipeline</p> <p>Goal: Build a basic producer \u2192 topic \u2192 consumer flow</p> <p>Time: ~30 minutes</p> <p>Skills: CLI tools, basic message flow</p> <p>Mini Project #2: Order Events System</p> <p>Goal: Publisher sends order events \u2192 Analytics consumer processes them</p> <p>Time: ~1 hour</p> <p>Skills: Programming producers/consumers, handling events</p>"},{"location":"part-1/#time-commitment","title":"\u23f1\ufe0f Time Commitment","text":"<ul> <li>Reading: ~2 hours</li> <li>Hands-on Labs: ~2 hours</li> <li>Total: ~4 hours to complete Part I</li> </ul>"},{"location":"part-1/#prerequisites","title":"\ud83c\udf93 Prerequisites","text":"<ul> <li>Basic programming knowledge</li> <li>Docker installed</li> <li>Code editor (VS Code recommended)</li> </ul> <p>Ready to Start?</p> <p>Begin with Chapter 1: Why Kafka? \u2192</p>"},{"location":"part-1/01-why-kafka/","title":"Why Kafka?","text":""},{"location":"part-1/01-why-kafka/#chapter-1-why-kafka-the-problem-it-solves","title":"Chapter 1: Why Kafka? The Problem It Solves","text":""},{"location":"part-1/01-why-kafka/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this chapter, you'll understand:</p> <ul> <li>Real-world problems that Kafka solves</li> <li>When to use Kafka (and when NOT to)</li> <li>How Kafka fits into modern architectures</li> <li>Practical use cases you'll recognize</li> </ul>"},{"location":"part-1/01-why-kafka/#the-problem-traditional-systems-break-at-scale","title":"\ud83d\udd0d The Problem: Traditional Systems Break at Scale","text":""},{"location":"part-1/01-why-kafka/#scenario-1-the-e-commerce-nightmare","title":"Scenario 1: The E-Commerce Nightmare","text":"<p>Imagine you're building an online store. When a customer places an order:</p> <pre><code>graph LR\n    A[Order Service] --&gt; B[Inventory Service]\n    A --&gt; C[Payment Service]\n    A --&gt; D[Notification Service]\n    A --&gt; E[Analytics Service]\n    A --&gt; F[Shipping Service]</code></pre> <p>Problems with direct API calls:</p> <p>Pain Points</p> <ul> <li>\ud83d\udd25 If one service is down, the whole order fails</li> <li>\ud83d\udc0c Slow services delay the entire flow</li> <li>\ud83d\udcc8 Scaling is a nightmare (tight coupling)</li> <li>\ud83d\udd04 Adding new services requires changing code everywhere</li> </ul>"},{"location":"part-1/01-why-kafka/#the-kafka-solution","title":"\ud83d\udca1 The Kafka Solution","text":"<p>Kafka acts as an event backbone \u2014 a central nervous system for your architecture.</p> <pre><code>graph LR\n    A[Order Service] --&gt; K[Kafka]\n    K --&gt; B[Inventory]\n    K --&gt; C[Payment]\n    K --&gt; D[Notifications]\n    K --&gt; E[Analytics]\n    K --&gt; F[Shipping]</code></pre> <p>Benefits:</p> <p>Kafka Advantages</p> <ul> <li>\u2705 Decoupling: Services don't know about each other</li> <li>\u2705 Resilience: Failed services can catch up later</li> <li>\u2705 Scalability: Add consumers without touching producers</li> <li>\u2705 Replay: Reprocess events anytime (like a DVR for data)</li> </ul>"},{"location":"part-1/01-why-kafka/#real-world-use-cases","title":"\ud83c\udf0d Real-World Use Cases","text":"E-CommerceFinanceIoTSocial Media <ul> <li>Order processing pipelines</li> <li>Inventory updates</li> <li>Customer activity tracking</li> </ul> <ul> <li>Real-time fraud detection</li> <li>Payment processing</li> <li>Transaction logging</li> </ul> <ul> <li>Sensor data ingestion</li> <li>Fleet monitoring</li> <li>Predictive maintenance</li> </ul> <ul> <li>Activity feeds</li> <li>Notifications</li> <li>Recommendation engines</li> </ul>"},{"location":"part-1/01-why-kafka/#when-not-to-use-kafka","title":"\u26a0\ufe0f When NOT to Use Kafka","text":"<p>Kafka Isn't Always the Answer</p> <p>Don't use Kafka if:</p> <ul> <li>You need request-response (use REST/gRPC)</li> <li>You have &lt; 100 events/day (overkill)</li> <li>Your data must be &lt; 1ms latency (Kafka is ~10ms+)</li> <li>You need transactions across services (use orchestration/saga)</li> </ul>"},{"location":"part-1/01-why-kafka/#kafka-vs-alternatives","title":"\ud83d\udcca Kafka vs Alternatives","text":"Feature Kafka RabbitMQ AWS SNS/SQS REST APIs Throughput \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Replay \u2705 Yes \u274c No \u274c No \u274c No Ordering \u2705 Per Partition \u26a0\ufe0f Limited \u274c No \u2705 Yes Complexity \ud83d\udd34 High \ud83d\udfe1 Medium \ud83d\udfe2 Low \ud83d\udfe2 Low"},{"location":"part-1/01-why-kafka/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<p>Remember</p> <ul> <li>Kafka is an event streaming platform, not just a message queue</li> <li>It decouples systems and enables event-driven architectures</li> <li>Best for high-throughput, replay-able event streams</li> <li>Not a replacement for REST APIs or databases</li> </ul> <p>Next Step</p> <p>Now that you know WHY Kafka, let's learn Core Concepts \u2192</p>"},{"location":"part-1/02-core-concepts/","title":"Core Concepts","text":""},{"location":"part-1/02-core-concepts/#chapter-2-core-concepts-you-must-know","title":"Chapter 2: Core Concepts You Must Know","text":""},{"location":"part-1/02-core-concepts/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>Master the essential Kafka building blocks:</p> <ul> <li>Topics, Partitions, Offsets</li> <li>Producers and Consumers</li> <li>Brokers and Clusters</li> <li>Consumer Groups</li> </ul>"},{"location":"part-1/02-core-concepts/#the-kafka-mental-model","title":"\ud83d\udcda The Kafka Mental Model","text":"<p>Think of Kafka as a distributed append-only log system.</p> <pre><code>graph TB\n    P[Producer] --&gt;|Write Events| T[Topic]\n    T --&gt; C1[Consumer 1]\n    T --&gt; C2[Consumer 2]\n    T --&gt; C3[Consumer 3]</code></pre>"},{"location":"part-1/02-core-concepts/#topics-event-categories","title":"\ud83d\uddc2\ufe0f Topics: Event Categories","text":"<p>A topic is like a folder or database table \u2014 it holds events of a specific type.</p> <p>Topic Examples</p> <ul> <li><code>orders</code> \u2014 all order events</li> <li><code>user-signups</code> \u2014 registration events</li> <li><code>payment-transactions</code> \u2014 payment events</li> </ul>"},{"location":"part-1/02-core-concepts/#creating-a-topic","title":"Creating a Topic","text":"<pre><code>kafka-topics --create --topic orders \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 \\\n  --replication-factor 1\n</code></pre>"},{"location":"part-1/02-core-concepts/#partitions-the-secret-to-scale","title":"\ud83d\uddc3\ufe0f Partitions: The Secret to Scale","text":"<p>Topics are split into partitions \u2014 parallel lanes for events.</p> <pre><code>Topic: orders\n\u251c\u2500\u2500 Partition 0: [msg0, msg3, msg6, ...]\n\u251c\u2500\u2500 Partition 1: [msg1, msg4, msg7, ...]\n\u2514\u2500\u2500 Partition 2: [msg2, msg5, msg8, ...]\n</code></pre> <p>Why Partitions?</p> <ul> <li>Parallelism: Multiple consumers read simultaneously</li> <li>Ordering: Guaranteed within a partition (not across partitions)</li> <li>Scalability: Add partitions to increase throughput</li> </ul>"},{"location":"part-1/02-core-concepts/#offsets-message-ids","title":"\ud83d\udd22 Offsets: Message IDs","text":"<p>Each message in a partition gets a unique offset (like an index).</p> <pre><code>Partition 0:\nOffset 0: {\"orderId\": \"123\", \"amount\": 99.99}\nOffset 1: {\"orderId\": \"124\", \"amount\": 149.99}\nOffset 2: {\"orderId\": \"125\", \"amount\": 49.99}\n</code></pre> <p>Consumers track their position using offsets.</p>"},{"location":"part-1/02-core-concepts/#producers-event-writers","title":"\ud83d\udce4 Producers: Event Writers","text":"<p>Producers publish events to topics.</p> <pre><code>from kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Send an event\nproducer.send('orders', {'orderId': '123', 'amount': 99.99})\nproducer.flush()\n</code></pre>"},{"location":"part-1/02-core-concepts/#producer-decisions","title":"Producer Decisions","text":"<p>Key Design Choices</p> <ol> <li>Which partition? \u2192 Kafka decides (round-robin) or you specify a key</li> <li>Guaranteed delivery? \u2192 Configure <code>acks</code> (0, 1, or all)</li> <li>Retry? \u2192 Set <code>retries</code> and <code>retry.backoff.ms</code></li> </ol>"},{"location":"part-1/02-core-concepts/#consumers-event-readers","title":"\ud83d\udce5 Consumers: Event Readers","text":"<p>Consumers subscribe to topics and process events.</p> <pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    'orders',\n    bootstrap_servers='localhost:9092',\n    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n    group_id='order-processors'\n)\n\nfor message in consumer:\n    order = message.value\n    print(f\"Processing order: {order['orderId']}\")\n</code></pre>"},{"location":"part-1/02-core-concepts/#consumer-groups-parallel-processing","title":"\ud83d\udc65 Consumer Groups: Parallel Processing","text":"<p>Multiple consumers with the same group ID share the workload.</p> <pre><code>Topic: orders (3 partitions)\nConsumer Group: order-processors\n\nConsumer 1 \u2192 reads Partition 0\nConsumer 2 \u2192 reads Partition 1\nConsumer 3 \u2192 reads Partition 2\n</code></pre> <p>Scaling Pattern</p> <ul> <li>Add more consumers (up to # of partitions) to increase throughput</li> <li>Each consumer in a group processes different partitions</li> <li>Different groups get all messages independently</li> </ul>"},{"location":"part-1/02-core-concepts/#brokers-clusters","title":"\ud83d\udda5\ufe0f Brokers &amp; Clusters","text":"<p>A broker is a Kafka server. A cluster is multiple brokers working together.</p> <pre><code>Cluster:\n\u251c\u2500\u2500 Broker 1 (leader for partition 0)\n\u251c\u2500\u2500 Broker 2 (leader for partition 1)\n\u2514\u2500\u2500 Broker 3 (leader for partition 2)\n</code></pre>"},{"location":"part-1/02-core-concepts/#replication","title":"Replication","text":"<p>Partitions are replicated across brokers for fault tolerance.</p> <pre><code>Partition 0:\n  - Leader: Broker 1\n  - Followers: Broker 2, Broker 3\n</code></pre> <p>If Broker 1 fails, Broker 2 becomes the new leader.</p>"},{"location":"part-1/02-core-concepts/#putting-it-all-together","title":"\ud83e\udde9 Putting It All Together","text":"<pre><code>graph TB\n    subgraph Producers\n        P1[Producer 1]\n        P2[Producer 2]\n    end\n\n    subgraph Kafka Cluster\n        B1[Broker 1]\n        B2[Broker 2]\n        B3[Broker 3]\n        T[Topic: orders&lt;br/&gt;3 Partitions]\n    end\n\n    subgraph Consumer Group\n        C1[Consumer 1]\n        C2[Consumer 2]\n    end\n\n    P1 --&gt; T\n    P2 --&gt; T\n    T --&gt; C1\n    T --&gt; C2</code></pre>"},{"location":"part-1/02-core-concepts/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<p>Remember</p> <ul> <li>Topics organize events by type</li> <li>Partitions enable parallelism and ordering</li> <li>Offsets track position in the log</li> <li>Producers write events; Consumers read them</li> <li>Consumer Groups enable scalable processing</li> <li>Brokers store and serve data with replication</li> </ul> <p>Next Step</p> <p>Ready to get hands-on? Let's do a 10-Minute Setup \u2192</p>"},{"location":"part-1/03-setup/","title":"10-Min Setup","text":""},{"location":"part-1/03-setup/#chapter-3-setup-in-10-minutes","title":"Chapter 3: Setup in 10 Minutes","text":""},{"location":"part-1/03-setup/#quick-start-with-docker","title":"\ud83c\udfaf Quick Start with Docker","text":"<p>Get Kafka running locally in minutes using Docker Compose.</p>"},{"location":"part-1/03-setup/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Docker Desktop installed</li> <li>8GB RAM available</li> <li>Terminal/Command line access</li> </ul>"},{"location":"part-1/03-setup/#step-1-docker-compose-setup","title":"\ud83d\ude80 Step 1: Docker Compose Setup","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n</code></pre> <p>Start Kafka:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"part-1/03-setup/#step-2-verify-installation","title":"\u2705 Step 2: Verify Installation","text":"<pre><code># Check running containers\ndocker ps\n\n# Should see zookeeper and kafka running\n</code></pre>"},{"location":"part-1/03-setup/#step-3-create-your-first-topic","title":"\ud83d\udcdd Step 3: Create Your First Topic","text":"<pre><code># Create a topic named \"test\"\ndocker exec -it &lt;kafka-container-id&gt; kafka-topics --create \\\n  --topic test \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 \\\n  --replication-factor 1\n\n# List topics\ndocker exec -it &lt;kafka-container-id&gt; kafka-topics --list \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"part-1/03-setup/#step-4-send-messages-producer","title":"\ud83d\udce4 Step 4: Send Messages (Producer)","text":"<pre><code># Start producer console\ndocker exec -it &lt;kafka-container-id&gt; kafka-console-producer \\\n  --topic test \\\n  --bootstrap-server localhost:9092\n\n# Type messages (press Enter after each):\n&gt; Hello Kafka!\n&gt; This is my first message\n&gt; Event-driven architecture is cool\n</code></pre> <p>Press <code>Ctrl+C</code> to exit.</p>"},{"location":"part-1/03-setup/#step-5-consume-messages","title":"\ud83d\udce5 Step 5: Consume Messages","text":"<p>Open a new terminal:</p> <pre><code># Start consumer from beginning\ndocker exec -it &lt;kafka-container-id&gt; kafka-console-consumer \\\n  --topic test \\\n  --from-beginning \\\n  --bootstrap-server localhost:9092\n</code></pre> <p>You should see your messages!</p>"},{"location":"part-1/03-setup/#mini-project-1-simple-event-pipeline","title":"\ud83c\udfaf Mini Project #1: Simple Event Pipeline","text":"<p>Goal: Build a basic producer \u2192 Kafka \u2192 consumer flow</p> <p>Tasks:</p> <ol> <li>Create a topic called <code>events</code></li> <li>Produce 10 messages with different data</li> <li>Consume them with a consumer group</li> <li>Verify all messages received</li> </ol> <p>Time: 15-30 minutes</p> Solution Commands <pre><code># Create topic\nkafka-topics --create --topic events \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 --replication-factor 1\n\n# Produce messages\nfor i in {1..10}; do\n  echo \"Event $i: User action at $(date)\" | \\\n  kafka-console-producer --topic events \\\n    --bootstrap-server localhost:9092\ndone\n\n# Consume\nkafka-console-consumer --topic events \\\n  --from-beginning \\\n  --bootstrap-server localhost:9092 \\\n  --group my-group\n</code></pre>"},{"location":"part-1/03-setup/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":"<p>Common Issues</p> <p>Port already in use: </p><pre><code># Check what's using port 9092\nlsof -i :9092  # Mac/Linux\nnetstat -ano | findstr :9092  # Windows\n</code></pre><p></p> <p>Kafka won't start: </p><pre><code># Check logs\ndocker logs &lt;kafka-container-id&gt;\n</code></pre><p></p> <p>Can't connect: - Ensure <code>KAFKA_ADVERTISED_LISTENERS</code> matches your setup - Check firewall settings</p> <p>Next Step</p> <p>Kafka is running! Now let's write code \u2192</p>"},{"location":"part-1/04-programming/","title":"Programming Kafka","text":""},{"location":"part-1/04-programming/#chapter-4-programming-with-kafka","title":"Chapter 4: Programming with Kafka","text":""},{"location":"part-1/04-programming/#write-your-first-producer-consumer","title":"\ud83c\udfaf Write Your First Producer &amp; Consumer","text":"<p>Learn to interact with Kafka using code (Java and Python examples provided).</p>"},{"location":"part-1/04-programming/#python-setup","title":"\ud83d\udc0d Python Setup","text":"<pre><code>pip install kafka-python\n</code></pre>"},{"location":"part-1/04-programming/#building-a-producer","title":"\ud83d\udce4 Building a Producer","text":""},{"location":"part-1/04-programming/#basic-producer-python","title":"Basic Producer (Python)","text":"<pre><code>from kafka import KafkaProducer\nimport json\nimport time\n\n# Create producer\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Send messages\nfor i in range(10):\n    event = {\n        'order_id': f'ORD-{i}',\n        'amount': 100 + i * 10,\n        'timestamp': time.time()\n    }\n\n    # Send to 'orders' topic\n    future = producer.send('orders', value=event)\n\n    # Wait for send to complete\n    record_metadata = future.get(timeout=10)\n\n    print(f\"Sent to partition {record_metadata.partition} \"\n          f\"at offset {record_metadata.offset}\")\n\nproducer.flush()\nproducer.close()\n</code></pre>"},{"location":"part-1/04-programming/#with-key-for-partitioning","title":"With Key (for Partitioning)","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    key_serializer=lambda k: k.encode('utf-8'),\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Messages with same key go to same partition\nproducer.send('orders', key='user-123', value={'order': 'data'})\n</code></pre>"},{"location":"part-1/04-programming/#building-a-consumer","title":"\ud83d\udce5 Building a Consumer","text":""},{"location":"part-1/04-programming/#basic-consumer-python","title":"Basic Consumer (Python)","text":"<pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    'orders',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',  # Start from beginning\n    group_id='order-processors',\n    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n)\n\nprint(\"Waiting for messages...\")\n\nfor message in consumer:\n    order = message.value\n    print(f\"Received: Order {order['order_id']}, \"\n          f\"Amount: ${order['amount']}\")\n\n    # Process order here\n    # ...\n\nconsumer.close()\n</code></pre>"},{"location":"part-1/04-programming/#with-manual-commit","title":"With Manual Commit","text":"<pre><code>consumer = KafkaConsumer(\n    'orders',\n    bootstrap_servers='localhost:9092',\n    group_id='order-processors',\n    enable_auto_commit=False,  # Manual commit\n    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n)\n\nfor message in consumer:\n    try:\n        # Process message\n        process_order(message.value)\n\n        # Commit offset manually\n        consumer.commit()\n    except Exception as e:\n        print(f\"Error: {e}\")\n        # Don't commit on error (will reprocess)\n</code></pre>"},{"location":"part-1/04-programming/#producer-configuration","title":"\u2699\ufe0f Producer Configuration","text":""},{"location":"part-1/04-programming/#acknowledgments-acks","title":"Acknowledgments (acks)","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    acks='all',  # 0, 1, or 'all'\n    retries=3,\n    max_in_flight_requests_per_connection=1  # Ordering guarantee\n)\n</code></pre> acks Behavior Use Case <code>0</code> Fire and forget Metrics, logs (OK to lose) <code>1</code> Leader acknowledges Balanced (default) <code>all</code> All replicas acknowledge Critical data"},{"location":"part-1/04-programming/#idempotent-producer","title":"\ud83d\udd04 Idempotent Producer","text":"<p>Ensures exactly-once semantics (no duplicates even with retries).</p> <pre><code>producer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    enable_idempotence=True,  # Prevents duplicates\n    acks='all',\n    retries=3\n)\n</code></pre>"},{"location":"part-1/04-programming/#mini-project-2-order-events-system","title":"\ud83c\udfaf Mini Project #2: Order Events System","text":"<p>Goal: Build a publisher that sends order events, and an analytics consumer that processes them.</p> <p>Architecture:</p> <pre><code>Order Service (Producer)\n        \u2193\n    orders topic\n        \u2193\nAnalytics Service (Consumer)\n</code></pre>"},{"location":"part-1/04-programming/#requirements","title":"Requirements","text":"<ol> <li>Producer: Generate 100 random orders</li> <li>Consumer: Calculate total revenue</li> <li>Consumer: Find average order value</li> <li>Consumer: Count orders per user</li> </ol> <p>Starter Code:</p> Producer (order_producer.py)Consumer (analytics_consumer.py) <pre><code>from kafka import KafkaProducer\nimport json\nimport random\nimport time\n\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\nusers = ['user-1', 'user-2', 'user-3', 'user-4', 'user-5']\n\nfor i in range(100):\n    order = {\n        'order_id': f'ORD-{i}',\n        'user_id': random.choice(users),\n        'amount': round(random.uniform(10, 500), 2),\n        'timestamp': time.time()\n    }\n\n    producer.send('orders', value=order)\n    print(f\"Sent: {order}\")\n    time.sleep(0.1)\n\nproducer.flush()\nproducer.close()\n</code></pre> <pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    'orders',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    group_id='analytics',\n    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n)\n\ntotal_revenue = 0\norder_count = 0\nuser_orders = {}\n\nfor message in consumer:\n    order = message.value\n\n    # Calculate metrics\n    total_revenue += order['amount']\n    order_count += 1\n\n    user_id = order['user_id']\n    user_orders[user_id] = user_orders.get(user_id, 0) + 1\n\n    # Print running totals\n    avg_value = total_revenue / order_count\n    print(f\"Orders: {order_count}, \"\n          f\"Revenue: ${total_revenue:.2f}, \"\n          f\"Avg: ${avg_value:.2f}\")\n\n    if order_count &gt;= 100:\n        break\n\nprint(\"\\n=== Final Analytics ===\")\nprint(f\"Total Revenue: ${total_revenue:.2f}\")\nprint(f\"Average Order Value: ${total_revenue/order_count:.2f}\")\nprint(f\"Orders per User: {user_orders}\")\n\nconsumer.close()\n</code></pre> <p>Run It:</p> <pre><code># Terminal 1: Start consumer\npython analytics_consumer.py\n\n# Terminal 2: Start producer\npython order_producer.py\n</code></pre>"},{"location":"part-1/04-programming/#java-examples","title":"\u2615 Java Examples","text":"Producer (Java)Consumer (Java) <pre><code>import org.apache.kafka.clients.producer.*;\nimport java.util.Properties;\n\npublic class OrderProducer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(\"bootstrap.servers\", \"localhost:9092\");\n        props.put(\"key.serializer\", \n            \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \n            \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"acks\", \"all\");\n\n        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\n\n        for (int i = 0; i &lt; 100; i++) {\n            String order = String.format(\n                \"{\\\"order_id\\\": \\\"ORD-%d\\\", \\\"amount\\\": %.2f}\",\n                i, 100 + i * 10.0\n            );\n\n            ProducerRecord&lt;String, String&gt; record = \n                new ProducerRecord&lt;&gt;(\"orders\", order);\n\n            producer.send(record, (metadata, exception) -&gt; {\n                if (exception != null) {\n                    exception.printStackTrace();\n                } else {\n                    System.out.printf(\"Sent to partition %d at offset %d%n\",\n                        metadata.partition(), metadata.offset());\n                }\n            });\n        }\n\n        producer.close();\n    }\n}\n</code></pre> <pre><code>import org.apache.kafka.clients.consumer.*;\nimport java.time.Duration;\nimport java.util.*;\n\npublic class OrderConsumer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(\"bootstrap.servers\", \"localhost:9092\");\n        props.put(\"group.id\", \"order-processors\");\n        props.put(\"key.deserializer\", \n            \"org.apache.kafka.common.serialization.StringDeserializer\");\n        props.put(\"value.deserializer\", \n            \"org.apache.kafka.common.serialization.StringDeserializer\");\n        props.put(\"auto.offset.reset\", \"earliest\");\n\n        Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n        consumer.subscribe(Collections.singletonList(\"orders\"));\n\n        while (true) {\n            ConsumerRecords&lt;String, String&gt; records = \n                consumer.poll(Duration.ofMillis(100));\n\n            for (ConsumerRecord&lt;String, String&gt; record : records) {\n                System.out.printf(\"Received: %s%n\", record.value());\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"part-1/04-programming/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<p>Remember</p> <ul> <li>Use acks='all' for critical data</li> <li>Enable idempotence to prevent duplicates</li> <li>Manual commit for at-least-once processing</li> <li>Use keys for partition control</li> <li>Handle exceptions gracefully</li> </ul> <p>Part I Complete!</p> <p>You've mastered the basics. Ready for Part II: Build Applications \u2192</p>"},{"location":"part-2/","title":"Part II: Build Applications","text":""},{"location":"part-2/#part-ii-build-applications","title":"Part II: Build Applications","text":""},{"location":"part-2/#design-develop-deploy-real-kafka-apps","title":"\ud83c\udfd7\ufe0f Design, Develop &amp; Deploy Real Kafka Apps","text":"<p>Now that you understand the basics, it's time to build production-grade applications. This section covers event design, schema management, stream processing, and integrations.</p>"},{"location":"part-2/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>\u2705 Design events and topics for real systems</li> <li>\u2705 Manage schemas with Avro, Protobuf, JSON</li> <li>\u2705 Build stream processing pipelines</li> <li>\u2705 Integrate Kafka with databases and data lakes</li> </ul>"},{"location":"part-2/#chapters","title":"\ud83d\uddfa\ufe0f Chapters","text":"<ul> <li> <p> Chapter 5: Event Design</p> <p>Schema design, naming conventions, partitioning strategies.</p> <p> Design Events</p> </li> <li> <p>:material-schema:{ .lg .middle } Chapter 6: Schema Management</p> <p>Avro, Protobuf, JSON Schema Registry in practice.</p> <p> Manage Schemas</p> </li> <li> <p> Chapter 7: Stream Processing</p> <p>KStream, KTable, windowing, joins, aggregations.</p> <p> Process Streams</p> </li> <li> <p> Chapter 8: Kafka Connect</p> <p>Database sync, S3 integration, Elasticsearch without code.</p> <p> Use Connect</p> </li> </ul>"},{"location":"part-2/#hands-on-projects","title":"\ud83c\udfaf Hands-On Projects","text":"<p>Mini Project #3: Versioned Events</p> <p>Goal: Evolve schemas without breaking consumers</p> <p>Time: ~1 hour</p> <p>Mini Project #4: Fraud Detection Pipeline</p> <p>Goal: Real-time fraud detection with Kafka Streams</p> <p>Time: ~2 hours</p> <p>Start Building</p> <p>Begin with Chapter 5: Event Design \u2192</p>"},{"location":"part-2/05-event-design/","title":"Event Design","text":""},{"location":"part-2/05-event-design/#chapter-5-designing-events-topics-for-real-systems","title":"Chapter 5: Designing Events &amp; Topics for Real Systems","text":""},{"location":"part-2/05-event-design/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Event schema design principles</li> <li>Topic naming conventions</li> <li>Partitioning strategies</li> <li>Retention policies</li> </ul>"},{"location":"part-2/05-event-design/#event-schema-design","title":"\ud83d\udcdd Event Schema Design","text":""},{"location":"part-2/05-event-design/#anatomy-of-a-good-event","title":"Anatomy of a Good Event","text":"<pre><code>{\n  \"event_id\": \"uuid-here\",\n  \"event_type\": \"order.created\",\n  \"event_version\": \"v1\",\n  \"timestamp\": \"2025-12-06T10:30:00Z\",\n  \"source\": \"order-service\",\n  \"data\": {\n    \"order_id\": \"ORD-123\",\n    \"user_id\": \"USER-456\",\n    \"amount\": 99.99,\n    \"items\": [...]\n  },\n  \"metadata\": {\n    \"correlation_id\": \"trace-id\",\n    \"causation_id\": \"parent-event-id\"\n  }\n}\n</code></pre> <p>Best Practices</p> <ul> <li>Include event metadata (ID, type, version, timestamp)</li> <li>Use ISO 8601 for timestamps</li> <li>Include source/origin information</li> <li>Add correlation/causation IDs for tracing</li> </ul>"},{"location":"part-2/05-event-design/#topic-naming-conventions","title":"\ud83c\udff7\ufe0f Topic Naming Conventions","text":"<pre><code>&lt;domain&gt;.&lt;entity&gt;.&lt;event-type&gt;\n\nExamples:\n- orders.order.created\n- orders.order.updated\n- payments.transaction.completed\n- inventory.product.stock-changed\n</code></pre>"},{"location":"part-2/05-event-design/#choosing-partition-keys","title":"\ud83d\udd11 Choosing Partition Keys","text":"Use Case Key Strategy Example Per-user ordering <code>user_id</code> All events for user-123 \u2192 same partition Per-order ordering <code>order_id</code> Order lifecycle events stay together Load balancing <code>null</code> Round-robin distribution Geography <code>region</code> Events by region"},{"location":"part-2/05-event-design/#real-example-e-commerce-order-topic","title":"\ud83c\udfaf Real Example: E-Commerce Order Topic","text":"<p>[Content to be expanded with detailed design example]</p> <p>Next</p> <p>Learn Schema Management \u2192</p>"},{"location":"part-2/06-schema-management/","title":"Schema Management","text":""},{"location":"part-2/06-schema-management/#chapter-6-schema-management-for-real-teams","title":"Chapter 6: Schema Management for Real Teams","text":""},{"location":"part-2/06-schema-management/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Schema formats (Avro, Protobuf, JSON Schema)</li> <li>Schema Registry usage</li> <li>Schema evolution strategies</li> </ul>"},{"location":"part-2/06-schema-management/#schema-formats-comparison","title":"\ud83d\udccb Schema Formats Comparison","text":"Format Pros Cons Best For Avro Compact, built-in evolution Binary Data pipelines Protobuf Fast, language support Learning curve Microservices JSON Schema Human-readable Larger size APIs, debugging"},{"location":"part-2/06-schema-management/#schema-registry","title":"\ud83d\uddc4\ufe0f Schema Registry","text":"<p>Central repository for managing schemas with versioning.</p> <pre><code>from confluent_kafka import avro\nfrom confluent_kafka.avro import AvroProducer\n\nvalue_schema = avro.loads('''\n{\n  \"type\": \"record\",\n  \"name\": \"Order\",\n  \"fields\": [\n    {\"name\": \"order_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"float\"}\n  ]\n}\n''')\n\nproducer = AvroProducer({\n    'bootstrap.servers': 'localhost:9092',\n    'schema.registry.url': 'http://localhost:8081'\n}, default_value_schema=value_schema)\n</code></pre>"},{"location":"part-2/06-schema-management/#schema-evolution","title":"\ud83d\udd04 Schema Evolution","text":""},{"location":"part-2/06-schema-management/#compatible-changes","title":"Compatible Changes \u2705","text":"<ul> <li>Adding optional fields</li> <li>Adding fields with defaults</li> <li>Removing fields with defaults</li> </ul>"},{"location":"part-2/06-schema-management/#incompatible-changes","title":"Incompatible Changes \u274c","text":"<ul> <li>Removing required fields</li> <li>Changing field types</li> <li>Renaming fields (without aliases)</li> </ul>"},{"location":"part-2/06-schema-management/#mini-project-3-versioned-events","title":"\ud83c\udfaf Mini Project #3: Versioned Events","text":"<p>[Hands-on project with schema evolution]</p> <p>Next</p> <p>Explore Stream Processing \u2192</p>"},{"location":"part-2/07-stream-processing/","title":"Stream Processing","text":""},{"location":"part-2/07-stream-processing/#chapter-7-stream-processing-that-matters","title":"Chapter 7: Stream Processing that Matters","text":""},{"location":"part-2/07-stream-processing/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Kafka Streams fundamentals</li> <li>KStream vs KTable</li> <li>Windowing operations</li> <li>Joins and aggregations</li> </ul>"},{"location":"part-2/07-stream-processing/#kafka-streams-basics","title":"\ud83c\udf0a Kafka Streams Basics","text":"<p>Kafka Streams = Library for building stream processing apps on Kafka.</p> <pre><code>StreamsBuilder builder = new StreamsBuilder();\nKStream&lt;String, Order&gt; orders = builder.stream(\"orders\");\n\n// Filter high-value orders\nKStream&lt;String, Order&gt; highValue = orders.filter(\n    (key, order) -&gt; order.getAmount() &gt; 1000\n);\n\nhighValue.to(\"high-value-orders\");\n</code></pre>"},{"location":"part-2/07-stream-processing/#kstream-vs-ktable","title":"\ud83d\udcca KStream vs KTable","text":"KStream KTable Event log Changelog (state) Insert-only Upserts (latest value) All events matter Current state matters"},{"location":"part-2/07-stream-processing/#windowing","title":"\u23f0 Windowing","text":"<p>Group events by time windows for aggregations.</p> <pre><code>KStream&lt;String, Order&gt; orders = builder.stream(\"orders\");\n\n// Tumbling window: non-overlapping 1-hour windows\nKTable&lt;Windowed&lt;String&gt;, Long&gt; ordersPerHour = orders\n    .groupByKey()\n    .windowedBy(TimeWindows.of(Duration.ofHours(1)))\n    .count();\n</code></pre>"},{"location":"part-2/07-stream-processing/#window-types","title":"Window Types","text":"<ul> <li>Tumbling: Fixed, non-overlapping (9-10am, 10-11am)</li> <li>Hopping: Fixed, overlapping (9-10am, 9:30-10:30am)</li> <li>Sliding: Event-driven windows</li> <li>Session: Activity-based gaps</li> </ul>"},{"location":"part-2/07-stream-processing/#mini-project-4-fraud-detection-pipeline","title":"\ud83c\udfaf Mini Project #4: Fraud Detection Pipeline","text":"<p>Goal: Detect suspicious patterns in transactions</p> <p>[Complete fraud detection example with windowing and alerts]</p> <p>Next</p> <p>Learn Kafka Connect \u2192</p>"},{"location":"part-2/08-kafka-connect/","title":"Kafka Connect","text":""},{"location":"part-2/08-kafka-connect/#chapter-8-kafka-connect-real-integrations","title":"Chapter 8: Kafka Connect \u2014 Real Integrations","text":""},{"location":"part-2/08-kafka-connect/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Kafka Connect fundamentals</li> <li>Source vs Sink connectors</li> <li>Real integrations (DB, S3, Elasticsearch)</li> </ul>"},{"location":"part-2/08-kafka-connect/#what-is-kafka-connect","title":"\ud83d\udd0c What is Kafka Connect?","text":"<p>Framework for streaming data between Kafka and external systems without writing code.</p> <pre><code>Database \u2192 Source Connector \u2192 Kafka \u2192 Sink Connector \u2192 Data Lake\n</code></pre>"},{"location":"part-2/08-kafka-connect/#source-connectors","title":"\ud83d\udce5 Source Connectors","text":"<p>Pull data into Kafka from external systems.</p>"},{"location":"part-2/08-kafka-connect/#jdbc-source-database-kafka","title":"JDBC Source (Database \u2192 Kafka)","text":"<pre><code>{\n  \"name\": \"mysql-source\",\n  \"config\": {\n    \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",\n    \"connection.url\": \"jdbc:mysql://localhost:3306/mydb\",\n    \"mode\": \"incrementing\",\n    \"incrementing.column.name\": \"id\",\n    \"topic.prefix\": \"mysql-\",\n    \"table.whitelist\": \"orders,customers\"\n  }\n}\n</code></pre>"},{"location":"part-2/08-kafka-connect/#sink-connectors","title":"\ud83d\udce4 Sink Connectors","text":"<p>Push data from Kafka to external systems.</p>"},{"location":"part-2/08-kafka-connect/#s3-sink-kafka-s3","title":"S3 Sink (Kafka \u2192 S3)","text":"<pre><code>{\n  \"name\": \"s3-sink\",\n  \"config\": {\n    \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",\n    \"topics\": \"orders\",\n    \"s3.bucket.name\": \"my-data-lake\",\n    \"s3.region\": \"us-east-1\",\n    \"format.class\": \"io.confluent.connect.s3.format.parquet.ParquetFormat\",\n    \"flush.size\": \"1000\"\n  }\n}\n</code></pre>"},{"location":"part-2/08-kafka-connect/#popular-connectors","title":"\ud83d\ude80 Popular Connectors","text":"Connector Type Use Case JDBC Source/Sink Database sync S3 Sink Data lake Elasticsearch Sink Search indexing MongoDB Source/Sink NoSQL integration Debezium Source CDC (Change Data Capture)"},{"location":"part-2/08-kafka-connect/#running-connect","title":"\ud83d\udee0\ufe0f Running Connect","text":"<pre><code># Start Kafka Connect\ndocker run -d \\\n  --name kafka-connect \\\n  -p 8083:8083 \\\n  confluentinc/cp-kafka-connect:7.5.0\n\n# Deploy a connector\ncurl -X POST http://localhost:8083/connectors \\\n  -H \"Content-Type: application/json\" \\\n  -d @connector-config.json\n</code></pre>"},{"location":"part-2/08-kafka-connect/#real-example-db-kafka-elasticsearch","title":"\ud83c\udfaf Real Example: DB \u2192 Kafka \u2192 Elasticsearch","text":"<p>[Complete ETL pipeline example]</p> <p>Part II Complete!</p> <p>Master Architecture Patterns next \u2192</p>"},{"location":"part-3/","title":"Part III: Architect Like a Pro","text":""},{"location":"part-3/#part-iii-architect-like-a-pro","title":"Part III: Architect Like a Pro","text":""},{"location":"part-3/#patterns-anti-patterns-real-architectures","title":"\ud83c\udfdb\ufe0f Patterns, Anti-Patterns &amp; Real Architectures","text":"<p>Learn to design production-grade systems with battle-tested patterns and avoid common pitfalls.</p>"},{"location":"part-3/#chapters","title":"\ud83d\uddfa\ufe0f Chapters","text":"<ul> <li> <p>:material-pattern:{ .lg .middle } Chapter 9: Design Patterns</p> <p>Event-driven microservices, fan-in/out, DLQ, replay, request-reply.</p> <p> Learn Patterns</p> </li> <li> <p> Chapter 10: Anti-Patterns</p> <p>Common mistakes and how to avoid them.</p> <p> Avoid Pitfalls</p> </li> <li> <p> Chapter 11: Real Architectures</p> <p>Payment systems, ride-hailing, clickstream, IoT pipelines.</p> <p> Study Architectures</p> </li> </ul> <p>Design Systems</p> <p>Begin with Chapter 9: Design Patterns \u2192</p>"},{"location":"part-3/09-design-patterns/","title":"Design Patterns","text":""},{"location":"part-3/09-design-patterns/#chapter-9-design-patterns","title":"Chapter 9: Design Patterns","text":""},{"location":"part-3/09-design-patterns/#essential-kafka-design-patterns","title":"\ud83c\udfaf Essential Kafka Design Patterns","text":"<p>Learn battle-tested patterns for building robust event-driven systems.</p>"},{"location":"part-3/09-design-patterns/#1-event-driven-microservices","title":"\ud83d\udd04 1. Event-Driven Microservices","text":"<p>Services communicate through events, not direct calls.</p> <pre><code>graph LR\n    A[Order Service] --&gt;|order.created| K[Kafka]\n    K --&gt;|subscribe| B[Inventory]\n    K --&gt;|subscribe| C[Notification]\n    K --&gt;|subscribe| D[Analytics]</code></pre> <p>Benefits: Loose coupling, scalability, resilience</p>"},{"location":"part-3/09-design-patterns/#2-fan-out-pattern","title":"\ud83d\udce2 2. Fan-Out Pattern","text":"<p>One event triggers multiple independent actions.</p> <pre><code>Order Created Event\n    \u251c\u2192 Send Email\n    \u251c\u2192 Update Inventory\n    \u251c\u2192 Process Payment\n    \u2514\u2192 Log Analytics\n</code></pre> <p>Use Case: Order processing, user registration</p>"},{"location":"part-3/09-design-patterns/#3-fan-in-pattern","title":"\ud83d\udce5 3. Fan-In Pattern","text":"<p>Multiple events aggregate into one stream.</p> <pre><code>User Clicks (Topic 1)\nPage Views (Topic 2)     \u2192 Combined Analytics Stream\nPurchases (Topic 3)\n</code></pre> <p>Use Case: Real-time dashboards, unified analytics</p>"},{"location":"part-3/09-design-patterns/#4-dead-letter-queue-dlq","title":"\u26b0\ufe0f 4. Dead Letter Queue (DLQ)","text":"<p>Failed messages go to a separate topic for later analysis.</p> <pre><code>try:\n    process_message(message)\nexcept Exception as e:\n    producer.send('orders-dlq', {\n        'original_message': message,\n        'error': str(e),\n        'timestamp': time.time()\n    })\n</code></pre> <p>Use Case: Error handling, debugging</p>"},{"location":"part-3/09-design-patterns/#5-event-replay-pattern","title":"\u23ee\ufe0f 5. Event Replay Pattern","text":"<p>Reprocess historical events (like rewinding a DVR).</p> <pre><code># Reset consumer group to beginning\nkafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --group my-group \\\n  --reset-offsets --to-earliest \\\n  --topic orders --execute\n</code></pre> <p>Use Cases: - Bug fixes (reprocess with corrected logic) - New features (replay old events through new consumers) - Data recovery</p>"},{"location":"part-3/09-design-patterns/#6-request-reply-with-kafka","title":"\ud83d\udcac 6. Request-Reply with Kafka","text":"<p>Implement synchronous-style communication over Kafka.</p> <pre><code># Requester\ncorrelation_id = str(uuid.uuid4())\nproducer.send('requests', {\n    'correlation_id': correlation_id,\n    'reply_topic': 'replies',\n    'data': {...}\n})\n\n# Wait for reply\nfor msg in consumer:\n    if msg.value['correlation_id'] == correlation_id:\n        return msg.value['response']\n</code></pre> <p>Use Case: Service orchestration, legacy system integration</p>"},{"location":"part-3/09-design-patterns/#pattern-selection-guide","title":"\ud83c\udf93 Pattern Selection Guide","text":"Pattern Use When Avoid When Event-Driven Async workflows Need immediate response Fan-Out Multiple downstream actions Tightly coupled logic Fan-In Aggregate from many sources Single source sufficient DLQ Transient failures expected Errors are show-stoppers Replay Events are immutable Data privacy concerns <p>Next</p> <p>Learn Anti-Patterns to Avoid \u2192</p>"},{"location":"part-3/10-anti-patterns/","title":"Anti-Patterns","text":""},{"location":"part-3/10-anti-patterns/#chapter-10-anti-patterns-to-avoid","title":"Chapter 10: Anti-Patterns to Avoid","text":""},{"location":"part-3/10-anti-patterns/#common-kafka-mistakes","title":"\ud83d\udea8 Common Kafka Mistakes","text":"<p>Learn from others' mistakes \u2014 avoid these common pitfalls.</p>"},{"location":"part-3/10-anti-patterns/#1-too-many-partitions","title":"\u274c 1. Too Many Partitions","text":"<p>Problem: Creating topics with 100+ partitions \"just in case\"</p> <p>Why Bad: - Increases broker memory/CPU - Slower leader elections - Rebalancing takes forever</p> <p>Don't Do This</p> <pre><code># Creating 1000 partitions for a small topic\nkafka-topics --create --topic logs \\\n  --partitions 1000  # Overkill!\n</code></pre> <p>Do This Instead</p> <pre><code># Start small, scale as needed\nkafka-topics --create --topic logs \\\n  --partitions 3  # Reasonable start\n</code></pre> <p>Rule of Thumb: Start with 3-6 partitions, scale based on throughput needs</p>"},{"location":"part-3/10-anti-patterns/#2-one-topic-per-entity-instance","title":"\u274c 2. One Topic Per Entity Instance","text":"<p>Problem: Creating <code>user-123-events</code>, <code>user-456-events</code>, etc.</p> <p>Why Bad: - Topic explosion (metadata overhead) - Hard to query/aggregate - Management nightmare</p> <p>Do This Instead</p> <p>Use one topic with partition keys </p><pre><code># All users in 'user-events', keyed by user_id\nproducer.send('user-events', key='user-123', value=event)\n</code></pre><p></p>"},{"location":"part-3/10-anti-patterns/#3-schema-chaos","title":"\u274c 3. Schema Chaos","text":"<p>Problem: No schema governance, everyone sends random JSON</p> <p>Results: - Field name inconsistencies (<code>userId</code> vs <code>user_id</code>) - Type mismatches (string vs int) - Breaking changes without notice</p> <p>Solution</p> <ul> <li>Use Schema Registry</li> <li>Enforce compatibility rules</li> <li>Version your schemas</li> </ul>"},{"location":"part-3/10-anti-patterns/#4-shared-state-in-consumers","title":"\u274c 4. Shared State in Consumers","text":"<p>Problem: Multiple consumer instances sharing mutable state</p> <pre><code># BAD: Shared global state\ntotal_revenue = 0  # Shared across instances!\n\nfor message in consumer:\n    total_revenue += message.value['amount']\n</code></pre> <p>Why Bad: Consumer group rebalancing causes data loss/corruption</p> <p>Do This Instead</p> <ul> <li>Use Kafka Streams state stores</li> <li>Use external databases</li> <li>Make consumers stateless</li> </ul>"},{"location":"part-3/10-anti-patterns/#5-ignoring-consumer-lag","title":"\u274c 5. Ignoring Consumer Lag","text":"<p>Problem: Not monitoring how far behind consumers are</p> <p>Consequences: - Delayed processing - Disk fills up (retention policy) - Business impact</p> <p>Solution</p> <pre><code># Monitor lag\nkafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --describe --group my-group\n</code></pre>"},{"location":"part-3/10-anti-patterns/#6-using-kafka-as-a-database","title":"\u274c 6. Using Kafka as a Database","text":"<p>Problem: Treating Kafka as primary storage</p> <p>Why Bad: - Not designed for random access queries - No indexes, joins, or ACID transactions - Retention policies may delete data</p> <p>Use Kafka For</p> <ul> <li>Event streaming</li> <li>Message passing</li> <li>Temporary buffering</li> </ul> <p>Use Databases For</p> <ul> <li>Querying by arbitrary fields</li> <li>Long-term storage</li> <li>ACID transactions</li> </ul>"},{"location":"part-3/10-anti-patterns/#7-large-messages","title":"\u274c 7. Large Messages","text":"<p>Problem: Sending multi-MB messages</p> <p>Consequences: - Network congestion - Memory pressure - Slow consumers</p> <p>Alternatives</p> <ol> <li>Claim Check Pattern: Store payload in S3, send reference</li> <li>Chunking: Split large payloads</li> <li>Compression: Enable producer compression</li> </ol>"},{"location":"part-3/10-anti-patterns/#8-no-error-handling","title":"\u274c 8. No Error Handling","text":"<p>Problem: Assuming messages always process successfully</p> <pre><code># BAD: No error handling\nfor message in consumer:\n    process(message.value)  # What if this fails?\n</code></pre> <p>Robust Pattern</p> <pre><code>for message in consumer:\n    try:\n        process(message.value)\n        consumer.commit()\n    except RecoverableError as e:\n        # Retry logic\n        retry_with_backoff(message)\n    except FatalError as e:\n        # Send to DLQ\n        send_to_dlq(message, error=e)\n        consumer.commit()  # Don't reprocess\n</code></pre>"},{"location":"part-3/10-anti-patterns/#anti-pattern-checklist","title":"\u2705 Anti-Pattern Checklist","text":"<p>Before going to production, verify:</p> <ul> <li> Partition count is reasonable (&lt; 50 per topic)</li> <li> Using partition keys properly (not creating topic per entity)</li> <li> Schema governance in place</li> <li> Consumers are stateless or use Kafka Streams</li> <li> Monitoring consumer lag</li> <li> Kafka is not the primary database</li> <li> Message sizes &lt; 1MB</li> <li> Error handling and DLQ implemented</li> </ul> <p>Next</p> <p>Study Real-World Architectures \u2192</p>"},{"location":"part-3/11-real-architectures/","title":"Real Architectures","text":""},{"location":"part-3/11-real-architectures/#chapter-11-real-world-architectures","title":"Chapter 11: Real-World Architectures","text":""},{"location":"part-3/11-real-architectures/#production-system-designs","title":"\ud83c\udfd7\ufe0f Production System Designs","text":"<p>Study battle-tested architectures from real companies.</p>"},{"location":"part-3/11-real-architectures/#1-payment-processing-system","title":"\ud83d\udcb3 1. Payment Processing System","text":"<pre><code>graph TB\n    A[API Gateway] --&gt;|payment.requested| K[Kafka]\n    K --&gt; B[Fraud Detection]\n    K --&gt; C[Payment Processor]\n    K --&gt; D[Notification Service]\n    C --&gt;|payment.completed| K\n    K --&gt; E[Accounting]\n    K --&gt; F[Analytics]</code></pre> <p>Key Patterns: - Event-driven orchestration - Idempotent processing (duplicate payments) - Dead letter queues for failed payments</p> <p>Topics: - <code>payments.requested</code> - <code>payments.validated</code> - <code>payments.completed</code> - <code>payments.failed</code></p>"},{"location":"part-3/11-real-architectures/#2-ride-hailing-event-platform","title":"\ud83d\ude97 2. Ride-Hailing Event Platform","text":"<pre><code>graph LR\n    subgraph Producers\n        D[Driver App]\n        R[Rider App]\n        M[Matching Service]\n    end\n\n    subgraph Kafka\n        T1[ride.requested]\n        T2[ride.matched]\n        T3[ride.completed]\n    end\n\n    subgraph Consumers\n        A[Analytics]\n        B[Billing]\n        C[Ratings]\n        N[Notifications]\n    end\n\n    D --&gt; T2\n    R --&gt; T1\n    M --&gt; T2\n    T1 --&gt; M\n    T2 --&gt; N\n    T3 --&gt; A\n    T3 --&gt; B\n    T3 --&gt; C</code></pre> <p>Scale Requirements: - 1M+ rides/day - Sub-second matching latency - Geographic partitioning</p> <p>Topics Strategy: - Partition by city/region (geo-locality) - Separate topics for different stages</p>"},{"location":"part-3/11-real-architectures/#3-clickstream-analytics-platform","title":"\ud83d\udcca 3. Clickstream Analytics Platform","text":"<pre><code>graph TB\n    W[Website/Mobile] --&gt;|clicks, views| K1[Kafka: raw-events]\n    K1 --&gt; S[Kafka Streams: Enrichment]\n    S --&gt;|enriched-events| K2[Kafka]\n    K2 --&gt; ES[Elasticsearch]\n    K2 --&gt; DW[Data Warehouse]\n    K2 --&gt; RT[Real-time Dashboard]</code></pre> <p>Data Flow: 1. Ingest: Millions of click events/sec 2. Enrich: Add user metadata, session data 3. Route: Different sinks for different use cases</p> <p>Partitioning: - By <code>user_id</code> (session analysis) - By <code>session_id</code> (event ordering)</p>"},{"location":"part-3/11-real-architectures/#4-iot-data-ingestion-pipeline","title":"\ud83c\udfed 4. IoT Data Ingestion Pipeline","text":"<pre><code>graph TB\n    subgraph Edge Devices\n        S1[Sensor 1]\n        S2[Sensor 2]\n        S3[Sensor N]\n    end\n\n    subgraph Kafka\n        T[telemetry-data&lt;br/&gt;100 partitions]\n    end\n\n    subgraph Processing\n        KS[Kafka Streams&lt;br/&gt;Anomaly Detection]\n        SP[Spark&lt;br/&gt;Batch Analytics]\n    end\n\n    subgraph Storage\n        TS[Time-Series DB]\n        DL[Data Lake]\n    end\n\n    S1 &amp; S2 &amp; S3 --&gt; T\n    T --&gt; KS\n    T --&gt; SP\n    KS --&gt; TS\n    SP --&gt; DL</code></pre> <p>Challenges: - High volume (10K+ devices \u00d7 1 msg/sec) - Out-of-order delivery - Device failures</p> <p>Solutions: - Windowed aggregations - Compression - Tiered storage</p>"},{"location":"part-3/11-real-architectures/#5-data-lake-ingestion","title":"\ud83c\udfe6 5. Data Lake Ingestion","text":"<pre><code>graph LR\n    subgraph Sources\n        DB1[(MySQL)]\n        DB2[(Postgres)]\n        API[External APIs]\n    end\n\n    subgraph Kafka\n        CDC[CDC Topics&lt;br/&gt;Debezium]\n        API_T[API Events]\n    end\n\n    subgraph Sinks\n        S3[S3/MinIO&lt;br/&gt;Parquet]\n        SF[Snowflake]\n        EL[Elasticsearch]\n    end\n\n    DB1 --&gt;|CDC| CDC\n    DB2 --&gt;|CDC| CDC\n    API --&gt; API_T\n    CDC --&gt; S3\n    CDC --&gt; SF\n    API_T --&gt; EL</code></pre> <p>CDC (Change Data Capture) Pattern: - Captures every database change as event - No impact on source databases - Enables real-time data warehouse</p>"},{"location":"part-3/11-real-architectures/#architecture-decision-guide","title":"\ud83d\udcc8 Architecture Decision Guide","text":"Requirement Pattern Example High throughput Many partitions, compression Clickstream Strict ordering Single partition or keyed Payments Geographic distribution Multi-cluster replication Ride-hailing Complex processing Kafka Streams Fraud detection Integration-heavy Kafka Connect Data lake"},{"location":"part-3/11-real-architectures/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<p>Lessons from Production</p> <ol> <li>Start simple, scale as needed</li> <li>Monitor everything (lag, throughput, errors)</li> <li>Design for failure (retries, DLQs, idempotence)</li> <li>Use the right tool (Kafka for streaming, DB for storage)</li> <li>Test at scale (chaos engineering)</li> </ol> <p>Part III Complete!</p> <p>Learn Production Deployment next \u2192</p>"},{"location":"part-4/","title":"Part IV: Deploy &amp; Run in Production","text":""},{"location":"part-4/#part-iv-deploy-run-in-production","title":"Part IV: Deploy &amp; Run in Production","text":""},{"location":"part-4/#run-kafka-at-scale","title":"\ud83d\ude80 Run Kafka at Scale","text":"<p>Learn deployment, monitoring, security, and optimization for production environments.</p>"},{"location":"part-4/#chapters","title":"\ud83d\uddfa\ufe0f Chapters","text":"<ul> <li> <p> Chapter 12: Production Deployment</p> <p>VM vs K8s vs Managed (Confluent/MSK), capacity planning.</p> <p> Deploy Kafka</p> </li> <li> <p> Chapter 13: Monitoring</p> <p>Metrics, lag debugging, exactly-once reality.</p> <p> Monitor Systems</p> </li> <li> <p> Chapter 14: Security</p> <p>SSL/TLS, ACLs, practical security policies.</p> <p> Secure Kafka</p> </li> <li> <p> Chapter 15: Scaling</p> <p>Performance tuning, rebalancing, tiered storage.</p> <p> Scale Up</p> </li> </ul> <p>Go Production</p> <p>Begin with Chapter 12: Production Deployment \u2192</p>"},{"location":"part-4/12-production/","title":"Running in Prod","text":""},{"location":"part-4/12-production/#chapter-12-running-kafka-in-production","title":"Chapter 12: Running Kafka in Production","text":""},{"location":"part-4/12-production/#deployment-options","title":"\ud83d\ude80 Deployment Options","text":"<p>Choose the right deployment model for your needs.</p>"},{"location":"part-4/12-production/#1-managed-services-easiest","title":"\u2601\ufe0f 1. Managed Services (Easiest)","text":""},{"location":"part-4/12-production/#confluent-cloud","title":"Confluent Cloud","text":"<p>Pros: - Fully managed (no ops overhead) - Auto-scaling - Built-in monitoring - Security by default</p> <p>Cons: - Cost ($$$$) - Vendor lock-in</p> <p>Best For: Teams wanting zero operational overhead</p>"},{"location":"part-4/12-production/#aws-msk-managed-streaming-for-kafka","title":"AWS MSK (Managed Streaming for Kafka)","text":"<p>Pros: - AWS integration - Managed infrastructure - Reasonable pricing</p> <p>Cons: - AWS-specific - Limited version upgrades</p> <p>Best For: AWS-native architectures</p>"},{"location":"part-4/12-production/#2-self-managed-on-vms","title":"\ud83d\udda5\ufe0f 2. Self-Managed on VMs","text":""},{"location":"part-4/12-production/#architecture","title":"Architecture","text":"<pre><code>Load Balancer\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Broker 1   \u2502  Broker 2   \u2502  Broker 3   \u2502\n\u2502  (Leader)   \u2502  (Follower) \u2502  (Follower) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193              \u2193              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ZooKeeper 1 \u2502 ZooKeeper 2 \u2502 ZooKeeper 3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Pros: - Full control - Cost-effective at scale - No vendor lock-in</p> <p>Cons: - You manage everything - Requires expertise</p>"},{"location":"part-4/12-production/#3-kubernetes-deployment","title":"\u2638\ufe0f 3. Kubernetes Deployment","text":""},{"location":"part-4/12-production/#using-strimzi-operator","title":"Using Strimzi Operator","text":"<pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: my-cluster\nspec:\n  kafka:\n    version: 3.5.0\n    replicas: 3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n    storage:\n      type: persistent-claim\n      size: 100Gi\n  zookeeper:\n    replicas: 3\n    storage:\n      type: persistent-claim\n      size: 10Gi\n</code></pre> <p>Pros: - Cloud-native - Auto-healing - Scalable</p> <p>Cons: - Kubernetes complexity - Storage performance considerations</p>"},{"location":"part-4/12-production/#capacity-planning","title":"\ud83d\udccf Capacity Planning","text":""},{"location":"part-4/12-production/#calculator","title":"Calculator","text":"<pre><code># Throughput requirements\nmessages_per_second = 10000\navg_message_size_kb = 1\nretention_days = 7\nreplication_factor = 3\n\n# Calculate disk needed\ndaily_data_gb = (messages_per_second * avg_message_size_kb * 86400) / 1024 / 1024\ntotal_disk_gb = daily_data_gb * retention_days * replication_factor\n\nprint(f\"Required disk space: {total_disk_gb:.2f} GB\")\n</code></pre>"},{"location":"part-4/12-production/#sizing-guidelines","title":"Sizing Guidelines","text":"Workload Brokers vCPUs/Broker RAM/Broker Disk/Broker Small 3 4 8 GB 500 GB Medium 5 8 16 GB 1 TB Large 7+ 16+ 32 GB+ 2+ TB SSD"},{"location":"part-4/12-production/#production-configuration","title":"\ud83d\udd27 Production Configuration","text":""},{"location":"part-4/12-production/#broker-config-serverproperties","title":"Broker Config (<code>server.properties</code>)","text":"<pre><code># Broker basics\nbroker.id=1\nlog.dirs=/var/lib/kafka/data\n\n# Networking\nlisteners=PLAINTEXT://:9092\nadvertised.listeners=PLAINTEXT://broker1.example.com:9092\n\n# Replication\ndefault.replication.factor=3\nmin.insync.replicas=2\nunclean.leader.election.enable=false\n\n# Performance\nnum.network.threads=8\nnum.io.threads=8\nsocket.send.buffer.bytes=102400\nsocket.receive.buffer.bytes=102400\n\n# Retention\nlog.retention.hours=168\nlog.segment.bytes=1073741824\nlog.retention.check.interval.ms=300000\n\n# Compression\ncompression.type=producer\n</code></pre>"},{"location":"part-4/12-production/#deployment-checklist","title":"\ud83c\udfaf Deployment Checklist","text":"<p>Pre-Production</p> <ul> <li> Minimum 3 brokers for HA</li> <li> <code>replication.factor &gt;= 3</code></li> <li> <code>min.insync.replicas = 2</code></li> <li> Monitoring configured (Prometheus + Grafana)</li> <li> Alerts set up (lag, disk, CPU)</li> <li> Backup strategy defined</li> <li> Disaster recovery plan documented</li> </ul> <p>Next</p> <p>Set up Monitoring &amp; Troubleshooting \u2192</p>"},{"location":"part-4/13-monitoring/","title":"Monitoring","text":""},{"location":"part-4/13-monitoring/#chapter-13-monitoring-troubleshooting","title":"Chapter 13: Monitoring &amp; Troubleshooting","text":""},{"location":"part-4/13-monitoring/#metrics-that-matter","title":"\ud83d\udcca Metrics That Matter","text":"<p>Focus on the metrics that actually indicate problems.</p>"},{"location":"part-4/13-monitoring/#critical-metrics","title":"\ud83c\udfaf Critical Metrics","text":""},{"location":"part-4/13-monitoring/#1-consumer-lag","title":"1. Consumer Lag","text":"<p>Most important metric \u2014 how far behind are consumers?</p> <pre><code># Check lag\nkafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --describe --group my-group\n\n# Output:\n# TOPIC    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG\n# orders   0          1000           1500            500  \u26a0\ufe0f Behind!\n# orders   1          2000           2000            0    \u2705 Caught up\n</code></pre> <p>When to Alert</p> <ul> <li>Lag &gt; 10,000 messages</li> <li>Lag increasing consistently</li> <li>Lag &gt; 5 minutes (time-based)</li> </ul>"},{"location":"part-4/13-monitoring/#2-broker-metrics","title":"2. Broker Metrics","text":"Metric Warning Threshold Critical CPU Usage &gt; 70% &gt; 90% Disk Usage &gt; 70% &gt; 85% Network I/O Near saturation Saturated Under-Replicated Partitions &gt; 0 &gt; 10"},{"location":"part-4/13-monitoring/#3-producer-metrics","title":"3. Producer Metrics","text":"<pre><code># Monitor producer metrics\nmetrics = producer.metrics()\n\n# Key metrics:\n# - record-send-rate\n# - record-error-rate\n# - request-latency-avg\n# - buffer-available-bytes\n</code></pre>"},{"location":"part-4/13-monitoring/#monitoring-stack","title":"\ud83d\udcc8 Monitoring Stack","text":""},{"location":"part-4/13-monitoring/#prometheus-grafana","title":"Prometheus + Grafana","text":"<pre><code># docker-compose.yml\nservices:\n  kafka-exporter:\n    image: danielqsj/kafka-exporter\n    ports:\n      - \"9308:9308\"\n    command:\n      - '--kafka.server=kafka:9092'\n\n  prometheus:\n    image: prom/prometheus\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n\n  grafana:\n    image: grafana/grafana\n    ports:\n      - \"3000:3000\"\n</code></pre> <p>Popular Dashboards: - Kafka Overview (ID: 7589) - Kafka Exporter (ID: 7589)</p>"},{"location":"part-4/13-monitoring/#debugging-consumer-lag","title":"\ud83d\udd0d Debugging Consumer Lag","text":""},{"location":"part-4/13-monitoring/#lag-investigation-steps","title":"Lag Investigation Steps","text":"<ol> <li> <p>Check if consumers are running </p><pre><code>kafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --describe --group my-group --members\n</code></pre><p></p> </li> <li> <p>Check consumer logs for errors </p><pre><code>docker logs my-consumer | grep ERROR\n</code></pre><p></p> </li> <li> <p>Measure consumer processing time </p><pre><code>import time\n\nfor message in consumer:\n    start = time.time()\n    process(message)\n    duration = time.time() - start\n    print(f\"Processed in {duration:.3f}s\")\n</code></pre><p></p> </li> <li> <p>Scale consumers if needed </p><pre><code># Add more consumer instances\ndocker-compose up --scale consumer=5\n</code></pre><p></p> </li> </ol>"},{"location":"part-4/13-monitoring/#common-issues-solutions","title":"\ud83d\udea8 Common Issues &amp; Solutions","text":""},{"location":"part-4/13-monitoring/#issue-1-slow-consumer","title":"Issue 1: Slow Consumer","text":"<p>Symptoms: Lag keeps increasing</p> <p>Solutions:</p> Scale HorizontallyOptimize ProcessingIncrease Partitions <p>Add more consumer instances (up to # of partitions)</p> <pre><code># Batch processing\nbatch = []\nfor message in consumer:\n    batch.append(message)\n    if len(batch) &gt;= 100:\n        process_batch(batch)\n        consumer.commit()\n        batch = []\n</code></pre> <pre><code>kafka-topics --alter --topic orders \\\n  --partitions 10 \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"part-4/13-monitoring/#issue-2-out-of-disk-space","title":"Issue 2: Out of Disk Space","text":"<p>Symptoms: Broker crashes, errors writing to log</p> <p>Solutions:</p> Reduce RetentionEnable CompressionAdd More Disks <pre><code>kafka-configs --alter \\\n  --entity-type topics \\\n  --entity-name orders \\\n  --add-config retention.ms=86400000 \\  # 1 day\n  --bootstrap-server localhost:9092\n</code></pre> <pre><code>producer = KafkaProducer(\n    compression_type='gzip'  # or 'snappy', 'lz4'\n)\n</code></pre> <p>Mount additional volumes to <code>log.dirs</code></p>"},{"location":"part-4/13-monitoring/#issue-3-rebalancing-storm","title":"Issue 3: Rebalancing Storm","text":"<p>Symptoms: Constant rebalancing, slow processing</p> <p>Root Causes: - <code>max.poll.interval.ms</code> too low - Consumer processing too slow - Network issues</p> <p>Solution: </p><pre><code>consumer = KafkaConsumer(\n    'orders',\n    max_poll_interval_ms=600000,  # 10 minutes\n    session_timeout_ms=45000,     # 45 seconds\n    heartbeat_interval_ms=15000   # 15 seconds\n)\n</code></pre><p></p>"},{"location":"part-4/13-monitoring/#at-least-once-vs-exactly-once","title":"\ud83c\udfaf At-Least-Once vs Exactly-Once","text":""},{"location":"part-4/13-monitoring/#at-least-once-default","title":"At-Least-Once (Default)","text":"<pre><code>for message in consumer:\n    process(message)\n    consumer.commit()  # After processing\n</code></pre> <p>Guarantees: No message loss Risk: Duplicates on failure</p>"},{"location":"part-4/13-monitoring/#exactly-once-transactions","title":"Exactly-Once (Transactions)","text":"<pre><code>producer = KafkaProducer(\n    transactional_id='my-transactional-id',\n    enable_idempotence=True\n)\n\nproducer.init_transactions()\nproducer.begin_transaction()\n\ntry:\n    producer.send('topic', message)\n    producer.commit_transaction()\nexcept:\n    producer.abort_transaction()\n</code></pre> <p>Guarantees: No duplicates Trade-off: Higher latency, complexity</p>"},{"location":"part-4/13-monitoring/#monitoring-checklist","title":"\ud83c\udf93 Monitoring Checklist","text":"<p>Essential Monitoring</p> <ul> <li> Consumer lag alerts</li> <li> Broker disk/CPU/memory alerts</li> <li> Under-replicated partitions alert</li> <li> Producer error rate alert</li> <li> Grafana dashboards set up</li> <li> Log aggregation (ELK, Splunk)</li> <li> Runbooks for common issues</li> </ul> <p>Next</p> <p>Implement Security \u2192</p>"},{"location":"part-4/14-security/","title":"Security","text":""},{"location":"part-4/14-security/#chapter-14-security-without-complexity","title":"Chapter 14: Security Without Complexity","text":""},{"location":"part-4/14-security/#essential-kafka-security","title":"\ud83d\udd12 Essential Kafka Security","text":"<p>Secure your Kafka cluster without over-engineering.</p>"},{"location":"part-4/14-security/#security-layers","title":"\ud83c\udfaf Security Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Authentication (who are you?)    \u2502  \u2190 SSL/SASL\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Authorization (what can you do?) \u2502  \u2190 ACLs\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Encryption (secure data)          \u2502  \u2190 SSL/TLS\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"part-4/14-security/#1-ssltls-encryption","title":"\ud83d\udd10 1. SSL/TLS Encryption","text":"<p>Encrypt data in transit.</p>"},{"location":"part-4/14-security/#generate-certificates","title":"Generate Certificates","text":"<pre><code># Create CA\nopenssl req -new -x509 -keyout ca-key -out ca-cert -days 365\n\n# Create broker keystore\nkeytool -genkey -keystore kafka.server.keystore.jks \\\n  -validity 365 -storepass password -keypass password \\\n  -dname \"CN=kafka-broker\" -storetype pkcs12\n\n# Sign certificate\nkeytool -certreq -keystore kafka.server.keystore.jks \\\n  -file cert-file -storepass password -keypass password\n\nopenssl x509 -req -CA ca-cert -CAkey ca-key \\\n  -in cert-file -out cert-signed -days 365\n</code></pre>"},{"location":"part-4/14-security/#broker-config","title":"Broker Config","text":"<pre><code># Enable SSL\nlisteners=SSL://:9093\nadvertised.listeners=SSL://broker1.example.com:9093\n\n# SSL settings\nssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks\nssl.keystore.password=password\nssl.key.password=password\nssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks\nssl.truststore.password=password\n</code></pre>"},{"location":"part-4/14-security/#producerconsumer-config","title":"Producer/Consumer Config","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='broker:9093',\n    security_protocol='SSL',\n    ssl_cafile='/path/to/ca-cert',\n    ssl_certfile='/path/to/client-cert',\n    ssl_keyfile='/path/to/client-key'\n)\n</code></pre>"},{"location":"part-4/14-security/#2-authentication-sasl","title":"\ud83d\udc64 2. Authentication (SASL)","text":"<p>Verify user identity.</p>"},{"location":"part-4/14-security/#saslplain-simple","title":"SASL/PLAIN (Simple)","text":"<p>Broker Config:</p> <pre><code>listeners=SASL_SSL://:9093\nsecurity.inter.broker.protocol=SASL_SSL\nsasl.mechanism.inter.broker.protocol=PLAIN\nsasl.enabled.mechanisms=PLAIN\n</code></pre> <p>JAAS Config (<code>kafka_server_jaas.conf</code>):</p> <pre><code>KafkaServer {\n  org.apache.kafka.common.security.plain.PlainLoginModule required\n  username=\"admin\"\n  password=\"admin-secret\"\n  user_admin=\"admin-secret\"\n  user_producer=\"producer-secret\"\n  user_consumer=\"consumer-secret\";\n};\n</code></pre> <p>Client Config:</p> <pre><code>producer = KafkaProducer(\n    bootstrap_servers='broker:9093',\n    security_protocol='SASL_SSL',\n    sasl_mechanism='PLAIN',\n    sasl_plain_username='producer',\n    sasl_plain_password='producer-secret'\n)\n</code></pre>"},{"location":"part-4/14-security/#saslscram-recommended","title":"SASL/SCRAM (Recommended)","text":"<p>More secure than PLAIN (hashed passwords).</p> <pre><code># Create user\nkafka-configs --bootstrap-server localhost:9092 \\\n  --alter --add-config 'SCRAM-SHA-256=[password=secret]' \\\n  --entity-type users --entity-name alice\n</code></pre>"},{"location":"part-4/14-security/#3-acls-authorization","title":"\ud83d\udee1\ufe0f 3. ACLs (Authorization)","text":"<p>Control who can do what.</p>"},{"location":"part-4/14-security/#grant-permissions","title":"Grant Permissions","text":"<pre><code># Producer write access\nkafka-acls --bootstrap-server localhost:9092 \\\n  --add --allow-principal User:producer \\\n  --operation Write --topic orders\n\n# Consumer read access\nkafka-acls --bootstrap-server localhost:9092 \\\n  --add --allow-principal User:consumer \\\n  --operation Read --topic orders \\\n  --group consumer-group-1\n\n# Admin access\nkafka-acls --bootstrap-server localhost:9092 \\\n  --add --allow-principal User:admin \\\n  --operation All --topic '*' --cluster\n</code></pre>"},{"location":"part-4/14-security/#list-acls","title":"List ACLs","text":"<pre><code>kafka-acls --bootstrap-server localhost:9092 \\\n  --list --topic orders\n</code></pre>"},{"location":"part-4/14-security/#practical-security-policies","title":"\ud83c\udfe2 Practical Security Policies","text":""},{"location":"part-4/14-security/#microservices-pattern","title":"Microservices Pattern","text":"<p>Each service gets: - Own user (e.g., <code>order-service</code>, <code>inventory-service</code>) - Minimal permissions (principle of least privilege)</p> <pre><code># Order Service\nkafka-acls --add --allow-principal User:order-service \\\n  --operation Write --topic orders\n\n# Inventory Service\nkafka-acls --add --allow-principal User:inventory-service \\\n  --operation Read --topic orders \\\n  --group inventory-group\n</code></pre>"},{"location":"part-4/14-security/#security-checklist","title":"\ud83d\udd0d Security Checklist","text":"<p>Production Security</p> <ul> <li> SSL/TLS enabled for all connections</li> <li> SASL authentication required</li> <li> ACLs enforced (not <code>allow.everyone</code>)</li> <li> Separate users per service</li> <li> Audit logging enabled</li> <li> Secrets in vault (not hardcoded)</li> <li> Network segmentation (VPC/firewall)</li> <li> ZooKeeper secured (if using)</li> </ul>"},{"location":"part-4/14-security/#quick-start-secure-kafka-with-docker","title":"\ud83d\ude80 Quick Start: Secure Kafka with Docker","text":"<pre><code>version: '3.8'\nservices:\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    environment:\n      KAFKA_LISTENERS: SASL_SSL://:9093\n      KAFKA_ADVERTISED_LISTENERS: SASL_SSL://kafka:9093\n      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL\n      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN\n      KAFKA_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka.keystore.jks\n      KAFKA_SSL_KEYSTORE_PASSWORD: password\n      KAFKA_OPTS: \"-Djava.security.auth.login.config=/etc/kafka/kafka_jaas.conf\"\n    volumes:\n      - ./secrets:/etc/kafka/secrets\n      - ./kafka_jaas.conf:/etc/kafka/kafka_jaas.conf\n</code></pre> <p>Next</p> <p>Learn Scaling &amp; Optimization \u2192</p>"},{"location":"part-4/15-scaling/","title":"Scaling","text":""},{"location":"part-4/15-scaling/#chapter-15-scaling-optimization","title":"Chapter 15: Scaling &amp; Optimization","text":""},{"location":"part-4/15-scaling/#scale-kafka-for-production-workloads","title":"\ud83d\udcc8 Scale Kafka for Production Workloads","text":"<p>Tune only what matters for performance.</p>"},{"location":"part-4/15-scaling/#performance-tuning-priorities","title":"\ud83c\udfaf Performance Tuning Priorities","text":"<ol> <li>Partitions (biggest impact)</li> <li>Batching (producer/consumer)</li> <li>Compression</li> <li>Hardware (disk, network)</li> </ol>"},{"location":"part-4/15-scaling/#1-partition-strategy","title":"\ud83d\udcca 1. Partition Strategy","text":""},{"location":"part-4/15-scaling/#scaling-throughput","title":"Scaling Throughput","text":"<pre><code>1 partition  \u2192  100 MB/s\n3 partitions \u2192  300 MB/s\n10 partitions \u2192 1 GB/s\n</code></pre> <p>Rule: More partitions = more parallelism</p> <p>But: Diminishing returns after ~50 partitions/topic</p>"},{"location":"part-4/15-scaling/#when-to-add-partitions","title":"When to Add Partitions","text":"<pre><code># Check current load\nkafka-run-class kafka.tools.GetOffsetShell \\\n  --broker-list localhost:9092 \\\n  --topic orders\n\n# Add partitions if needed\nkafka-topics --alter --topic orders \\\n  --partitions 10 \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"part-4/15-scaling/#2-producer-optimization","title":"\ud83d\ude80 2. Producer Optimization","text":""},{"location":"part-4/15-scaling/#batching-biggest-win","title":"Batching (Biggest Win)","text":"<pre><code>producer = KafkaProducer(\n    linger_ms=10,           # Wait 10ms to batch messages\n    batch_size=16384,       # 16KB batches\n    buffer_memory=33554432  # 32MB buffer\n)\n</code></pre> <p>Effect: 10-100x throughput increase</p>"},{"location":"part-4/15-scaling/#compression","title":"Compression","text":"<pre><code>producer = KafkaProducer(\n    compression_type='snappy'  # or 'gzip', 'lz4', 'zstd'\n)\n</code></pre> Compression Speed Ratio CPU snappy \u26a1\u26a1\u26a1 \ud83d\udddc\ufe0f Low lz4 \u26a1\u26a1\u26a1 \ud83d\udddc\ufe0f\ud83d\udddc\ufe0f Low gzip \u26a1 \ud83d\udddc\ufe0f\ud83d\udddc\ufe0f\ud83d\udddc\ufe0f High zstd \u26a1\u26a1 \ud83d\udddc\ufe0f\ud83d\udddc\ufe0f\ud83d\udddc\ufe0f Medium <p>Recommendation: <code>snappy</code> or <code>lz4</code> for most cases</p>"},{"location":"part-4/15-scaling/#idempotence-acks","title":"Idempotence &amp; Acks","text":"<pre><code># For critical data\nproducer = KafkaProducer(\n    enable_idempotence=True,  # Exactly-once\n    acks='all',               # Wait for all replicas\n    retries=3\n)\n\n# For high-throughput\nproducer = KafkaProducer(\n    acks=1,  # Only leader acknowledges\n    retries=0\n)\n</code></pre>"},{"location":"part-4/15-scaling/#3-consumer-optimization","title":"\ud83d\udce5 3. Consumer Optimization","text":""},{"location":"part-4/15-scaling/#batching","title":"Batching","text":"<pre><code>consumer = KafkaConsumer(\n    max_poll_records=500,        # Fetch 500 messages at once\n    fetch_min_bytes=1024,        # Wait for 1KB\n    fetch_max_wait_ms=500        # Or wait 500ms\n)\n\n# Process in batches\nbatch = []\nfor message in consumer:\n    batch.append(message)\n    if len(batch) &gt;= 100:\n        process_batch(batch)\n        consumer.commit()\n        batch = []\n</code></pre>"},{"location":"part-4/15-scaling/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\nexecutor = ThreadPoolExecutor(max_workers=10)\n\ndef process_message(msg):\n    # Heavy processing\n    time.sleep(0.1)\n    return result\n\nfor message in consumer:\n    executor.submit(process_message, message)\n</code></pre>"},{"location":"part-4/15-scaling/#4-broker-tuning","title":"\ud83d\udcbe 4. Broker Tuning","text":""},{"location":"part-4/15-scaling/#disk-performance","title":"Disk Performance","text":"<pre><code># Use XFS or ext4 filesystem\n# Mount with noatime\n# Use RAID 10 for production\n\n# Broker config\nnum.io.threads=16              # More threads for disk I/O\nlog.flush.interval.messages=10000\nlog.flush.interval.ms=1000\n</code></pre>"},{"location":"part-4/15-scaling/#network-tuning","title":"Network Tuning","text":"<pre><code># Increase network buffer\nsocket.send.buffer.bytes=1048576    # 1MB\nsocket.receive.buffer.bytes=1048576  # 1MB\nsocket.request.max.bytes=104857600   # 100MB max request\n\n# More network threads\nnum.network.threads=8\n</code></pre>"},{"location":"part-4/15-scaling/#5-avoiding-rebalance-pain","title":"\ud83d\udd04 5. Avoiding Rebalance Pain","text":""},{"location":"part-4/15-scaling/#problem-rebalancing-slows-everything","title":"Problem: Rebalancing Slows Everything","text":"<p>Causes: - Consumer processing too slow - Heartbeat timeout - New consumers joining</p>"},{"location":"part-4/15-scaling/#solution-tune-timeouts","title":"Solution: Tune Timeouts","text":"<pre><code>consumer = KafkaConsumer(\n    max_poll_interval_ms=600000,  # 10 minutes\n    session_timeout_ms=45000,     # 45 seconds\n    heartbeat_interval_ms=15000,  # 15 seconds (1/3 of session timeout)\n    max_poll_records=500          # Don't fetch too many\n)\n</code></pre>"},{"location":"part-4/15-scaling/#static-group-membership","title":"Static Group Membership","text":"<p>Prevents rebalancing on consumer restart.</p> <pre><code>consumer = KafkaConsumer(\n    group_id='my-group',\n    group_instance_id='consumer-1'  # Static ID\n)\n</code></pre>"},{"location":"part-4/15-scaling/#6-tiered-storage-kafka-36","title":"\ud83d\uddc4\ufe0f 6. Tiered Storage (Kafka 3.6+)","text":"<p>Offload old data to cheaper storage (S3/Azure Blob).</p> <pre><code># Enable tiered storage\nremote.log.storage.system.enable=true\nremote.log.manager.task.interval.ms=30000\n\n# Retention: Keep 1 day local, 7 days remote\nlog.retention.ms=86400000\nlog.local.retention.ms=86400000\n</code></pre> <p>Benefits: - 90% disk cost reduction - Infinite retention - Fast broker startup</p>"},{"location":"part-4/15-scaling/#performance-benchmarking","title":"\ud83d\udcca Performance Benchmarking","text":""},{"location":"part-4/15-scaling/#test-producer-throughput","title":"Test Producer Throughput","text":"<pre><code>kafka-producer-perf-test \\\n  --topic test \\\n  --num-records 1000000 \\\n  --record-size 1000 \\\n  --throughput -1 \\\n  --producer-props bootstrap.servers=localhost:9092 \\\n    acks=1 \\\n    compression.type=snappy\n</code></pre>"},{"location":"part-4/15-scaling/#test-consumer-throughput","title":"Test Consumer Throughput","text":"<pre><code>kafka-consumer-perf-test \\\n  --topic test \\\n  --messages 1000000 \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"part-4/15-scaling/#scaling-checklist","title":"\ud83c\udfaf Scaling Checklist","text":"<p>Before Scaling</p> <ul> <li> Identify bottleneck (producer, consumer, broker, disk)</li> <li> Enable compression</li> <li> Tune batching parameters</li> <li> Add partitions if needed</li> <li> Scale consumers horizontally</li> <li> Monitor metrics after changes</li> </ul>"},{"location":"part-4/15-scaling/#scale-milestones","title":"\ud83d\udcc8 Scale Milestones","text":"Throughput Brokers Partitions/Topic Notes &lt; 10 MB/s 3 3-6 Small deployment 10-100 MB/s 5-7 10-20 Medium scale 100 MB/s - 1 GB/s 10+ 30-50 Large scale &gt; 1 GB/s 20+ 100+ Enterprise <p>Part IV Complete!</p> <p>Build Real Projects next \u2192</p>"},{"location":"part-5/","title":"Part V: Learn by Building","text":""},{"location":"part-5/#part-v-learn-by-building","title":"Part V: Learn by Building","text":""},{"location":"part-5/#5-production-grade-projects","title":"\ud83d\udee0\ufe0f 5 Production-Grade Projects","text":"<p>Each project includes architecture diagrams, complete code, and Docker Compose files.</p>"},{"location":"part-5/#projects-overview","title":"\ud83c\udfaf Projects Overview","text":"# Project Duration What You'll Learn 1 Real-time Order Tracking 2-3 hours Topics, producers, consumers, basic patterns 2 Inventory Sync System 3-4 hours Event-driven microservices, choreography 3 Fraud Analytics Pipeline 4-5 hours Kafka Streams, windowing, aggregations 4 ETL to Data Warehouse 3-4 hours Kafka Connect, database sync, S3 5 Clickstream Analytics 4-5 hours End-to-end pipeline, dashboards"},{"location":"part-5/#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>Each project provides:</p> <ul> <li>\u2705 Architecture diagrams (system design)</li> <li>\u2705 Complete source code (Java/Python)</li> <li>\u2705 Docker Compose setup (run everything locally)</li> <li>\u2705 Step-by-step guide (build it yourself)</li> <li>\u2705 Testing scenarios (verify it works)</li> <li>\u2705 Extension ideas (take it further)</li> </ul>"},{"location":"part-5/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li> <p> Project 1: Order Tracking</p> <p>Build a real-time order tracking system with Kafka.</p> <p> Start Project</p> </li> <li> <p> Project 2: Inventory Sync</p> <p>Sync inventory across microservices with events.</p> <p> Start Project</p> </li> <li> <p> Project 3: Fraud Analytics</p> <p>Real-time fraud detection with streaming analytics.</p> <p> Start Project</p> </li> <li> <p> Project 4: ETL Pipeline</p> <p>Move data from databases to warehouse via Kafka.</p> <p> Start Project</p> </li> <li> <p> Project 5: Clickstream</p> <p>Build clickstream analytics with dashboards.</p> <p> Start Project</p> </li> </ul>"},{"location":"part-5/#recommended-approach","title":"\ud83d\udca1 Recommended Approach","text":"<p>How to Use These Projects</p> <ol> <li>Read the architecture first to understand the design</li> <li>Try building it yourself before looking at the solution</li> <li>Run the provided code to see it in action</li> <li>Modify and extend to solidify your learning</li> </ol> <p>Start Building</p> <p>Begin with Project 1: Order Tracking \u2192</p>"},{"location":"part-5/project-1-order-tracking/","title":"Order Tracking","text":""},{"location":"part-5/project-1-order-tracking/#project-1-real-time-order-tracking-system","title":"Project 1: Real-time Order Tracking System","text":""},{"location":"part-5/project-1-order-tracking/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>Build an end-to-end order tracking system where customers can see real-time updates as their orders move through different stages.</p> <p>Duration: 2-3 hours Difficulty: \u2b50\u2b50\u2606\u2606\u2606 (Beginner)</p>"},{"location":"part-5/project-1-order-tracking/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    A[Order API] --&gt;|order.created| K[Kafka: orders]\n    W[Warehouse System] --&gt;|order.packed| K\n    S[Shipping System] --&gt;|order.shipped| K\n    D[Delivery System] --&gt;|order.delivered| K\n\n    K --&gt; C1[Notification Service]\n    K --&gt; C2[Analytics Service]\n    K --&gt; C3[Customer Dashboard]\n\n    C1 --&gt;|email/SMS| U[Customer]\n    C3 --&gt;|WebSocket| UI[Web UI]</code></pre>"},{"location":"part-5/project-1-order-tracking/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li> Topic design for event-driven workflows</li> <li> Producer implementation (multiple services)</li> <li> Consumer groups for parallel processing</li> <li> Event ordering within partitions</li> <li> Docker Compose for local development</li> </ul>"},{"location":"part-5/project-1-order-tracking/#tech-stack","title":"\ud83d\udd27 Tech Stack","text":"<ul> <li>Backend: Python (FastAPI) or Java (Spring Boot)</li> <li>Kafka: Docker Compose</li> <li>Frontend: React (optional)</li> <li>Database: PostgreSQL</li> </ul>"},{"location":"part-5/project-1-order-tracking/#step-1-design-events","title":"\ud83d\udcdd Step 1: Design Events","text":""},{"location":"part-5/project-1-order-tracking/#order-event-schema","title":"Order Event Schema","text":"<pre><code>{\n  \"event_id\": \"uuid\",\n  \"event_type\": \"order.created | order.packed | order.shipped | order.delivered\",\n  \"timestamp\": \"2025-12-06T10:30:00Z\",\n  \"order_id\": \"ORD-12345\",\n  \"customer_id\": \"CUST-789\",\n  \"data\": {\n    \"status\": \"created\",\n    \"location\": \"warehouse-01\",\n    \"estimated_delivery\": \"2025-12-08\",\n    \"items\": [...]\n  }\n}\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#topic-strategy","title":"Topic Strategy","text":"<pre><code>Topic: orders\nPartitions: 3\nPartition Key: order_id (ensures all events for an order go to same partition)\nRetention: 7 days\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#step-2-setup-infrastructure","title":"\ud83d\udcbb Step 2: Setup Infrastructure","text":""},{"location":"part-5/project-1-order-tracking/#docker-compose","title":"Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n  postgres:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: postgres\n    ports:\n      - \"5432:5432\"\n</code></pre> <p>Start: </p><pre><code>docker-compose up -d\n</code></pre><p></p>"},{"location":"part-5/project-1-order-tracking/#step-3-build-producers","title":"\ud83d\udce4 Step 3: Build Producers","text":""},{"location":"part-5/project-1-order-tracking/#order-api-order_servicepy","title":"Order API (order_service.py)","text":"<pre><code>from kafka import KafkaProducer\nfrom fastapi import FastAPI\nimport json\nimport uuid\nfrom datetime import datetime\n\napp = FastAPI()\n\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    key_serializer=lambda k: k.encode('utf-8'),\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n@app.post(\"/orders\")\ndef create_order(order_data: dict):\n    order_id = f\"ORD-{uuid.uuid4().hex[:8]}\"\n\n    event = {\n        \"event_id\": str(uuid.uuid4()),\n        \"event_type\": \"order.created\",\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"order_id\": order_id,\n        \"customer_id\": order_data['customer_id'],\n        \"data\": {\n            \"status\": \"created\",\n            \"items\": order_data['items'],\n            \"total\": order_data['total']\n        }\n    }\n\n    # Key by order_id for ordering\n    producer.send('orders', key=order_id, value=event)\n    producer.flush()\n\n    return {\"order_id\": order_id, \"status\": \"created\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#warehouse-service-warehouse_servicepy","title":"Warehouse Service (warehouse_service.py)","text":"<pre><code>import time\nimport random\n\ndef simulate_warehouse():\n    \"\"\"Simulates packing orders\"\"\"\n    consumer = KafkaConsumer(\n        'orders',\n        bootstrap_servers='localhost:9092',\n        group_id='warehouse',\n        value_deserializer=lambda m: json.loads(m.decode())\n    )\n\n    for message in consumer:\n        event = message.value\n\n        if event['event_type'] == 'order.created':\n            # Simulate packing delay\n            time.sleep(random.uniform(2, 5))\n\n            packed_event = {\n                \"event_id\": str(uuid.uuid4()),\n                \"event_type\": \"order.packed\",\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"order_id\": event['order_id'],\n                \"customer_id\": event['customer_id'],\n                \"data\": {\"status\": \"packed\", \"location\": \"warehouse-01\"}\n            }\n\n            producer.send('orders', key=event['order_id'], value=packed_event)\n            print(f\"\u2705 Packed: {event['order_id']}\")\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#step-4-build-consumers","title":"\ud83d\udce5 Step 4: Build Consumers","text":""},{"location":"part-5/project-1-order-tracking/#notification-service","title":"Notification Service","text":"<pre><code>def notification_service():\n    \"\"\"Sends notifications for order updates\"\"\"\n    consumer = KafkaConsumer(\n        'orders',\n        bootstrap_servers='localhost:9092',\n        group_id='notifications',\n        value_deserializer=lambda m: json.loads(m.decode())\n    )\n\n    for message in consumer:\n        event = message.value\n        customer_id = event['customer_id']\n        status = event['event_type'].split('.')[1]\n\n        # Send notification (email/SMS)\n        print(f\"\ud83d\udce7 Notifying {customer_id}: Order {event['order_id']} is {status}\")\n\n        # In real app: send_email() or send_sms()\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#analytics-service","title":"Analytics Service","text":"<pre><code>def analytics_service():\n    \"\"\"Track order metrics\"\"\"\n    consumer = KafkaConsumer(\n        'orders',\n        bootstrap_servers='localhost:9092',\n        group_id='analytics',\n        value_deserializer=lambda m: json.loads(m.decode())\n    )\n\n    stats = {\n        'created': 0,\n        'packed': 0,\n        'shipped': 0,\n        'delivered': 0\n    }\n\n    for message in consumer:\n        event = message.value\n        status = event['event_type'].split('.')[1]\n        stats[status] += 1\n\n        print(f\"\ud83d\udcca Stats: {stats}\")\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#step-5-test-the-system","title":"\ud83e\uddea Step 5: Test the System","text":""},{"location":"part-5/project-1-order-tracking/#test-script","title":"Test Script","text":"<pre><code>import requests\nimport time\n\n# Create 10 orders\nfor i in range(10):\n    response = requests.post('http://localhost:8000/orders', json={\n        'customer_id': f'CUST-{i}',\n        'items': [{'sku': 'ITEM-1', 'qty': 2}],\n        'total': 99.99\n    })\n    print(f\"Created: {response.json()}\")\n    time.sleep(1)\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#run-all-services","title":"Run All Services","text":"<pre><code># Terminal 1: Start Kafka\ndocker-compose up\n\n# Terminal 2: Order API\npython order_service.py\n\n# Terminal 3: Warehouse simulator\npython warehouse_service.py\n\n# Terminal 4: Notification consumer\npython notification_service.py\n\n# Terminal 5: Analytics consumer\npython analytics_service.py\n\n# Terminal 6: Create orders\npython test_orders.py\n</code></pre>"},{"location":"part-5/project-1-order-tracking/#success-criteria","title":"\ud83c\udfaf Success Criteria","text":"<ul> <li> Orders created via API</li> <li> Events published to Kafka</li> <li> Warehouse picks up and packs orders</li> <li> Notifications sent for each status change</li> <li> Analytics tracks order counts</li> <li> All events for same order in same partition</li> </ul>"},{"location":"part-5/project-1-order-tracking/#extensions","title":"\ud83d\ude80 Extensions","text":"<ol> <li>Add Web UI \u2014 Real-time dashboard with WebSocket</li> <li>Add Shipping/Delivery \u2014 Complete the workflow</li> <li>Add Error Handling \u2014 Dead letter queue for failures</li> <li>Add Persistence \u2014 Store order state in PostgreSQL</li> <li>Add Monitoring \u2014 Prometheus metrics</li> </ol>"},{"location":"part-5/project-1-order-tracking/#complete-code","title":"\ud83d\udce6 Complete Code","text":"<p>Download full project \u2192</p> <p>Next Project</p> <p>Build Project 2: Inventory Sync \u2192</p>"},{"location":"part-5/project-2-inventory-sync/","title":"Inventory Sync","text":""},{"location":"part-5/project-2-inventory-sync/#project-2-inventory-sync-across-microservices","title":"Project 2: Inventory Sync Across Microservices","text":""},{"location":"part-5/project-2-inventory-sync/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>Build an event-driven inventory management system where multiple microservices stay in sync through Kafka events.</p> <p>Duration: 3-4 hours Difficulty: \u2b50\u2b50\u2b50\u2606\u2606 (Intermediate)</p>"},{"location":"part-5/project-2-inventory-sync/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    O[Order Service] --&gt;|order.placed| K[Kafka]\n    K --&gt;|consume| I[Inventory Service]\n    I --&gt;|inventory.reserved| K\n    I --&gt;|inventory.released| K\n    K --&gt; W[Warehouse Service]\n    K --&gt; A[Analytics Service]</code></pre>"},{"location":"part-5/project-2-inventory-sync/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>Event-driven choreography</li> <li>Saga pattern for distributed transactions</li> <li>Compensating transactions</li> <li>Event sourcing basics</li> <li>Idempotent consumers</li> </ul>"},{"location":"part-5/project-2-inventory-sync/#implementation","title":"\ud83d\udcbb Implementation","text":""},{"location":"part-5/project-2-inventory-sync/#events","title":"Events","text":"<ul> <li><code>inventory.reserve-requested</code></li> <li><code>inventory.reserved</code></li> <li><code>inventory.insufficient</code></li> <li><code>inventory.released</code></li> </ul>"},{"location":"part-5/project-2-inventory-sync/#services","title":"Services","text":"<ol> <li>Order Service \u2014 Initiates reservations</li> <li>Inventory Service \u2014 Manages stock levels</li> <li>Warehouse Service \u2014 Physical fulfillment</li> <li>Compensation Service \u2014 Handles rollbacks</li> </ol>"},{"location":"part-5/project-2-inventory-sync/#challenges","title":"\ud83c\udfaf Challenges","text":"<ul> <li>Handle concurrent reservations</li> <li>Implement idempotency (same request twice)</li> <li>Compensating actions on failures</li> </ul> <p>[Full implementation details to be added]</p> <p>Next Project</p> <p>Build Project 3: Fraud Analytics \u2192</p>"},{"location":"part-5/project-3-fraud-analytics/","title":"Fraud Analytics","text":""},{"location":"part-5/project-3-fraud-analytics/#project-3-fraud-detection-analytics-pipeline","title":"Project 3: Fraud Detection Analytics Pipeline","text":""},{"location":"part-5/project-3-fraud-analytics/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>Build a real-time fraud detection system using Kafka Streams with windowing and pattern matching.</p> <p>Duration: 4-5 hours Difficulty: \u2b50\u2b50\u2b50\u2b50\u2606 (Advanced)</p>"},{"location":"part-5/project-3-fraud-analytics/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    T[Transaction API] --&gt;|transactions| K[Kafka]\n    K --&gt; KS[Kafka Streams&lt;br/&gt;Fraud Detection]\n    KS --&gt;|suspicious| A[Alerts Topic]\n    KS --&gt;|approved| P[Processed Topic]\n    A --&gt; N[Notification Service]\n    A --&gt; D[Dashboard]</code></pre>"},{"location":"part-5/project-3-fraud-analytics/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>Kafka Streams fundamentals</li> <li>Windowing (tumbling, hopping, session)</li> <li>Stateful processing (aggregations)</li> <li>Pattern detection</li> <li>Real-time alerts</li> </ul>"},{"location":"part-5/project-3-fraud-analytics/#fraud-rules","title":"\ud83d\udea8 Fraud Rules","text":"<ol> <li>Velocity Check: &gt; 5 transactions in 10 minutes</li> <li>Amount Check: Transaction &gt; $10,000</li> <li>Location Change: Different countries within 1 hour</li> <li>Pattern Matching: Repeated small amounts (structuring)</li> </ol>"},{"location":"part-5/project-3-fraud-analytics/#kafka-streams-implementation","title":"\ud83d\udcbb Kafka Streams Implementation","text":"<pre><code>KStream&lt;String, Transaction&gt; transactions = builder.stream(\"transactions\");\n\n// Rule 1: Velocity check\nKTable&lt;Windowed&lt;String&gt;, Long&gt; txnCount = transactions\n    .groupByKey()\n    .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))\n    .count();\n\nKStream&lt;Windowed&lt;String&gt;, Long&gt; suspicious = txnCount\n    .toStream()\n    .filter((key, count) -&gt; count &gt; 5);\n\nsuspicious.to(\"fraud-alerts\");\n</code></pre> <p>[Full implementation with all rules to be added]</p> <p>Next Project</p> <p>Build Project 4: ETL Pipeline \u2192</p>"},{"location":"part-5/project-4-etl-pipeline/","title":"ETL Pipeline","text":""},{"location":"part-5/project-4-etl-pipeline/#project-4-etl-pipeline-to-data-warehouse","title":"Project 4: ETL Pipeline to Data Warehouse","text":""},{"location":"part-5/project-4-etl-pipeline/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>Build a CDC-based ETL pipeline using Kafka Connect to stream database changes to a data warehouse.</p> <p>Duration: 3-4 hours Difficulty: \u2b50\u2b50\u2b50\u2606\u2606 (Intermediate)</p>"},{"location":"part-5/project-4-etl-pipeline/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph LR\n    DB[(MySQL/Postgres)] --&gt;|CDC| D[Debezium]\n    D --&gt; K[Kafka]\n    K --&gt; T[Transformations]\n    T --&gt; S3[S3/MinIO]\n    T --&gt; SF[(Snowflake/Redshift)]\n    T --&gt; ES[Elasticsearch]</code></pre>"},{"location":"part-5/project-4-etl-pipeline/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>Change Data Capture (CDC)</li> <li>Debezium configuration</li> <li>Kafka Connect setup</li> <li>Data transformations</li> <li>Multiple sink connectors</li> </ul>"},{"location":"part-5/project-4-etl-pipeline/#components","title":"\ud83d\udd27 Components","text":""},{"location":"part-5/project-4-etl-pipeline/#1-source-debezium-mysql-cdc","title":"1. Source: Debezium (MySQL CDC)","text":"<pre><code>{\n  \"name\": \"mysql-source\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n    \"database.hostname\": \"mysql\",\n    \"database.port\": \"3306\",\n    \"database.user\": \"debezium\",\n    \"database.password\": \"dbz\",\n    \"database.server.id\": \"184054\",\n    \"database.server.name\": \"mysql\",\n    \"database.include.list\": \"inventory\",\n    \"database.history.kafka.bootstrap.servers\": \"kafka:9092\",\n    \"database.history.kafka.topic\": \"schema-changes\"\n  }\n}\n</code></pre>"},{"location":"part-5/project-4-etl-pipeline/#2-sink-s3","title":"2. Sink: S3","text":"<pre><code>{\n  \"name\": \"s3-sink\",\n  \"config\": {\n    \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",\n    \"topics\": \"mysql.inventory.orders\",\n    \"s3.bucket.name\": \"data-lake\",\n    \"format.class\": \"io.confluent.connect.s3.format.parquet.ParquetFormat\"\n  }\n}\n</code></pre>"},{"location":"part-5/project-4-etl-pipeline/#pipeline-flow","title":"\ud83c\udfaf Pipeline Flow","text":"<ol> <li>Database changes captured by Debezium</li> <li>CDC events published to Kafka</li> <li>Transformations applied (e.g., flatten nested JSON)</li> <li>Data routed to multiple sinks (S3, Snowflake, ES)</li> </ol> <p>[Full implementation with docker-compose to be added]</p> <p>Next Project</p> <p>Build Project 5: Clickstream Analytics \u2192</p>"},{"location":"part-5/project-5-clickstream/","title":"Clickstream","text":""},{"location":"part-5/project-5-clickstream/#project-5-clickstream-analytics-dashboard","title":"Project 5: Clickstream Analytics Dashboard","text":""},{"location":"part-5/project-5-clickstream/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>Build an end-to-end clickstream analytics pipeline from web tracking to real-time dashboard.</p> <p>Duration: 4-5 hours Difficulty: \u2b50\u2b50\u2b50\u2b50\u2b50 (Advanced)</p>"},{"location":"part-5/project-5-clickstream/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    W[Website] --&gt;|clicks| API[Tracking API]\n    API --&gt; K1[Kafka: raw-clicks]\n    K1 --&gt; KS[Kafka Streams&lt;br/&gt;Enrichment]\n    KS --&gt; K2[Kafka: enriched-clicks]\n    K2 --&gt; ES[Elasticsearch]\n    K2 --&gt; PG[(PostgreSQL)]\n    ES --&gt; G[Grafana Dashboard]\n    PG --&gt; G</code></pre>"},{"location":"part-5/project-5-clickstream/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>High-volume event ingestion</li> <li>Stream enrichment (joining streams)</li> <li>Sessionization (window aggregation)</li> <li>Multiple data sinks</li> <li>Real-time visualization</li> </ul>"},{"location":"part-5/project-5-clickstream/#metrics-to-track","title":"\ud83d\udcca Metrics to Track","text":"<ol> <li>Page Views \u2014 Total and per page</li> <li>Unique Users \u2014 Daily/hourly active users</li> <li>Session Duration \u2014 Average time on site</li> <li>Conversion Funnel \u2014 Homepage \u2192 Product \u2192 Checkout</li> <li>Popular Pages \u2014 Top 10 pages</li> <li>Real-time Traffic \u2014 Current users online</li> </ol>"},{"location":"part-5/project-5-clickstream/#implementation-highlights","title":"\ud83d\udcbb Implementation Highlights","text":""},{"location":"part-5/project-5-clickstream/#click-event-schema","title":"Click Event Schema","text":"<pre><code>{\n  \"event_id\": \"uuid\",\n  \"timestamp\": \"2025-12-06T10:30:00Z\",\n  \"user_id\": \"anon-12345\",\n  \"session_id\": \"sess-abc\",\n  \"page\": \"/products/item-123\",\n  \"referrer\": \"https://google.com\",\n  \"user_agent\": \"Mozilla/5.0...\",\n  \"ip\": \"203.0.113.1\"\n}\n</code></pre>"},{"location":"part-5/project-5-clickstream/#kafka-streams-sessionization","title":"Kafka Streams: Sessionization","text":"<pre><code>KStream&lt;String, ClickEvent&gt; clicks = builder.stream(\"raw-clicks\");\n\n// Session windows (10 min gap)\nKTable&lt;Windowed&lt;String&gt;, Long&gt; sessions = clicks\n    .groupByKey()\n    .windowedBy(SessionWindows.with(Duration.ofMinutes(10)))\n    .count();\n</code></pre>"},{"location":"part-5/project-5-clickstream/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>[Screenshot of real-time dashboard to be added]</p>"},{"location":"part-5/project-5-clickstream/#success-criteria","title":"\ud83c\udfaf Success Criteria","text":"<ul> <li> Ingest 1000+ clicks/sec</li> <li> Real-time dashboard updates</li> <li> Session tracking with windowing</li> <li> Conversion funnel analysis</li> <li> Historical data in warehouse</li> </ul> <p>[Full implementation with code and dashboard config to be added]</p> <p>Course Complete!</p> <p>Congratulations! Review Resources \u2192</p>"},{"location":"resources/cheat-sheets/","title":"Cheat Sheets","text":""},{"location":"resources/cheat-sheets/#kafka-cheat-sheets","title":"Kafka Cheat Sheets","text":""},{"location":"resources/cheat-sheets/#quick-reference","title":"\ud83d\ude80 Quick Reference","text":"<p>Essential commands and configurations for daily Kafka operations.</p>"},{"location":"resources/cheat-sheets/#topic-management","title":"\ud83d\udccb Topic Management","text":"<pre><code># Create topic\nkafka-topics --create \\\n  --topic my-topic \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 \\\n  --replication-factor 1\n\n# List topics\nkafka-topics --list --bootstrap-server localhost:9092\n\n# Describe topic\nkafka-topics --describe --topic my-topic --bootstrap-server localhost:9092\n\n# Delete topic\nkafka-topics --delete --topic my-topic --bootstrap-server localhost:9092\n\n# Alter topic (add partitions)\nkafka-topics --alter --topic my-topic --partitions 6 --bootstrap-server localhost:9092\n</code></pre>"},{"location":"resources/cheat-sheets/#producer-commands","title":"\ud83d\udce4 Producer Commands","text":"<pre><code># Console producer\nkafka-console-producer --topic my-topic --bootstrap-server localhost:9092\n\n# With key\nkafka-console-producer --topic my-topic \\\n  --property \"parse.key=true\" \\\n  --property \"key.separator=:\" \\\n  --bootstrap-server localhost:9092\n\n# Performance test\nkafka-producer-perf-test \\\n  --topic test \\\n  --num-records 1000000 \\\n  --record-size 1000 \\\n  --throughput -1 \\\n  --producer-props bootstrap.servers=localhost:9092\n</code></pre>"},{"location":"resources/cheat-sheets/#consumer-commands","title":"\ud83d\udce5 Consumer Commands","text":"<pre><code># Console consumer (from beginning)\nkafka-console-consumer --topic my-topic \\\n  --from-beginning \\\n  --bootstrap-server localhost:9092\n\n# With group\nkafka-console-consumer --topic my-topic \\\n  --group my-group \\\n  --bootstrap-server localhost:9092\n\n# With key\nkafka-console-consumer --topic my-topic \\\n  --property print.key=true \\\n  --property key.separator=\":\" \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"resources/cheat-sheets/#consumer-group-management","title":"\ud83d\udc65 Consumer Group Management","text":"<pre><code># List groups\nkafka-consumer-groups --list --bootstrap-server localhost:9092\n\n# Describe group (check lag)\nkafka-consumer-groups --describe --group my-group --bootstrap-server localhost:9092\n\n# Reset offsets to beginning\nkafka-consumer-groups --reset-offsets --to-earliest \\\n  --group my-group --topic my-topic \\\n  --execute --bootstrap-server localhost:9092\n\n# Reset to specific offset\nkafka-consumer-groups --reset-offsets --to-offset 100 \\\n  --group my-group --topic my-topic:0 \\\n  --execute --bootstrap-server localhost:9092\n</code></pre>"},{"location":"resources/cheat-sheets/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"resources/cheat-sheets/#producer-config-python","title":"Producer Config (Python)","text":"<pre><code>from kafka import KafkaProducer\n\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:9092',\n    acks='all',                    # 0, 1, or 'all'\n    retries=3,\n    max_in_flight_requests_per_connection=1,\n    compression_type='snappy',     # snappy, gzip, lz4, zstd\n    batch_size=16384,              # 16KB\n    linger_ms=10,                  # Wait 10ms to batch\n    buffer_memory=33554432,        # 32MB\n    enable_idempotence=True\n)\n</code></pre>"},{"location":"resources/cheat-sheets/#consumer-config-python","title":"Consumer Config (Python)","text":"<pre><code>from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(\n    'my-topic',\n    bootstrap_servers='localhost:9092',\n    group_id='my-group',\n    auto_offset_reset='earliest',  # 'earliest' or 'latest'\n    enable_auto_commit=True,\n    auto_commit_interval_ms=5000,\n    max_poll_records=500,\n    max_poll_interval_ms=300000,   # 5 minutes\n    session_timeout_ms=10000,\n    heartbeat_interval_ms=3000\n)\n</code></pre>"},{"location":"resources/cheat-sheets/#security","title":"\ud83d\udee1\ufe0f Security","text":""},{"location":"resources/cheat-sheets/#ssl-config","title":"SSL Config","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='broker:9093',\n    security_protocol='SSL',\n    ssl_cafile='/path/to/ca-cert',\n    ssl_certfile='/path/to/client-cert',\n    ssl_keyfile='/path/to/client-key'\n)\n</code></pre>"},{"location":"resources/cheat-sheets/#sasl-config","title":"SASL Config","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='broker:9093',\n    security_protocol='SASL_SSL',\n    sasl_mechanism='PLAIN',\n    sasl_plain_username='user',\n    sasl_plain_password='password'\n)\n</code></pre>"},{"location":"resources/cheat-sheets/#monitoring","title":"\ud83d\udcca Monitoring","text":"<pre><code># Check broker logs\ndocker logs kafka-broker\n\n# JMX metrics\nkafka-run-class kafka.tools.JmxTool \\\n  --object-name kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec\n\n# Get offsets\nkafka-run-class kafka.tools.GetOffsetShell \\\n  --broker-list localhost:9092 \\\n  --topic my-topic\n</code></pre>"},{"location":"resources/cheat-sheets/#common-patterns","title":"\ud83d\udd11 Common Patterns","text":""},{"location":"resources/cheat-sheets/#at-least-once-processing","title":"At-Least-Once Processing","text":"<pre><code>for message in consumer:\n    try:\n        process(message)\n        consumer.commit()\n    except Exception as e:\n        log_error(e)\n        # Message will be reprocessed\n</code></pre>"},{"location":"resources/cheat-sheets/#exactly-once-transactions","title":"Exactly-Once (Transactions)","text":"<pre><code>producer.init_transactions()\nproducer.begin_transaction()\ntry:\n    producer.send('topic', message)\n    producer.commit_transaction()\nexcept:\n    producer.abort_transaction()\n</code></pre>"},{"location":"resources/cheat-sheets/#docker-compose","title":"\ud83d\udc33 Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n</code></pre>"},{"location":"resources/cheat-sheets/#download","title":"\ud83d\udce5 Download","text":"<p>Download PDF Cheat Sheet \u2192</p> <p>More Resources</p> <p>Check out Tools &amp; Links and FAQ</p>"},{"location":"resources/faq/","title":"FAQ","text":""},{"location":"resources/faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"resources/faq/#common-kafka-questions","title":"\ud83e\udd14 Common Kafka Questions","text":""},{"location":"resources/faq/#getting-started","title":"Getting Started","text":""},{"location":"resources/faq/#q-do-i-need-zookeeper","title":"Q: Do I need ZooKeeper?","text":"<p>A: Depends on Kafka version:</p> <ul> <li>Kafka &lt; 3.3: Yes, ZooKeeper required</li> <li>Kafka 3.3+: Can use KRaft mode (ZooKeeper-free)</li> <li>Kafka 4.0+: ZooKeeper deprecated</li> </ul> <p>Recommendation: Use ZooKeeper for now (more stable), migrate to KRaft later.</p>"},{"location":"resources/faq/#q-how-many-partitions-should-i-create","title":"Q: How many partitions should I create?","text":"<p>A: Start small, scale as needed:</p> <ul> <li>Small workload: 3-6 partitions</li> <li>Medium: 10-20 partitions</li> <li>Large: 30-50+ partitions</li> </ul> <p>Rule: Don't exceed 50 partitions/topic unless necessary.</p> <p>Why: Each partition adds broker overhead.</p>"},{"location":"resources/faq/#q-whats-a-good-message-size","title":"Q: What's a good message size?","text":"<p>A:  - Ideal: &lt; 1 MB - Max: 1 MB (default broker limit) - Larger: Use claim-check pattern (store in S3, send reference)</p>"},{"location":"resources/faq/#architecture","title":"Architecture","text":""},{"location":"resources/faq/#q-kafka-vs-rabbitmq","title":"Q: Kafka vs RabbitMQ?","text":"Feature Kafka RabbitMQ Use Case Event streaming, high throughput Task queues, complex routing Replay \u2705 Yes \u274c No Throughput Very high (100K+ msg/s) Moderate (10K msg/s) Ordering Per partition Per queue Best For Event logs, analytics Work distribution <p>TLDR: Use Kafka for event streaming, RabbitMQ for task queues.</p>"},{"location":"resources/faq/#q-can-i-use-kafka-as-a-database","title":"Q: Can I use Kafka as a database?","text":"<p>A: \u274c No. Kafka is not a database.</p> <p>Use Kafka for: - Streaming data - Event logs - Temporary buffering</p> <p>Use Database for: - ACID transactions - Complex queries (JOIN, WHERE) - Long-term storage with indexes</p> <p>Pattern: Store events in Kafka, persist state in database.</p>"},{"location":"resources/faq/#operations","title":"Operations","text":""},{"location":"resources/faq/#q-how-do-i-monitor-consumer-lag","title":"Q: How do I monitor consumer lag?","text":"<pre><code>kafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --describe --group my-group\n</code></pre> <p>Alert thresholds: - \u26a0\ufe0f Warning: Lag &gt; 10,000 messages - \ud83d\udea8 Critical: Lag &gt; 100,000 or increasing consistently</p>"},{"location":"resources/faq/#q-my-consumer-is-slow-how-to-scale","title":"Q: My consumer is slow. How to scale?","text":"<p>Options:</p> <ol> <li>Add more consumer instances (up to # of partitions)</li> <li>Increase partitions (requires rebalancing)</li> <li>Optimize processing (batch, parallel)</li> <li>Tune <code>max.poll.records</code> (fetch more messages)</li> </ol>"},{"location":"resources/faq/#q-whats-the-retention-limit","title":"Q: What's the retention limit?","text":"<p>A: Configurable per topic:</p> <pre><code># Default: 7 days\nlog.retention.hours=168\n\n# Unlimited (use with tiered storage)\nlog.retention.hours=-1\n</code></pre> <p>Disk space = throughput \u00d7 retention period \u00d7 replication factor</p>"},{"location":"resources/faq/#performance","title":"Performance","text":""},{"location":"resources/faq/#q-how-to-improve-producer-throughput","title":"Q: How to improve producer throughput?","text":"<p>Top optimizations:</p> <ol> <li> <p>Batching: </p><pre><code>linger_ms=10  # Wait 10ms to batch\nbatch_size=16384  # 16KB batches\n</code></pre><p></p> </li> <li> <p>Compression: </p><pre><code>compression_type='snappy'\n</code></pre><p></p> </li> <li> <p>Async sending: </p><pre><code>producer.send('topic', msg)  # Don't wait for .get()\n</code></pre><p></p> </li> <li> <p>Reduce acks: </p><pre><code>acks=1  # Only leader (faster than 'all')\n</code></pre><p></p> </li> </ol>"},{"location":"resources/faq/#q-at-least-once-vs-exactly-once","title":"Q: At-least-once vs exactly-once?","text":"Guarantee Behavior Use When At-least-once May process duplicates Most cases (default) Exactly-once No duplicates Financial transactions <p>At-least-once (default): </p><pre><code>for msg in consumer:\n    process(msg)\n    consumer.commit()  # May reprocess on failure\n</code></pre><p></p> <p>Exactly-once (transactions): </p><pre><code>producer.init_transactions()\nproducer.begin_transaction()\nproducer.send('topic', msg)\nproducer.commit_transaction()\n</code></pre><p></p> <p>Trade-off: Exactly-once has higher latency.</p>"},{"location":"resources/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resources/faq/#q-not-enough-replicas-error","title":"Q: \"Not enough replicas\" error","text":"<p>Cause: <code>min.insync.replicas</code> &gt; available brokers</p> <p>Solution: </p><pre><code># Reduce min ISR\nkafka-configs --alter --topic my-topic \\\n  --add-config min.insync.replicas=1 \\\n  --bootstrap-server localhost:9092\n</code></pre><p></p> <p>Or: Add more brokers.</p>"},{"location":"resources/faq/#q-consumer-rebalancing-constantly","title":"Q: Consumer rebalancing constantly","text":"<p>Causes: - Processing too slow - <code>max.poll.interval.ms</code> too low - Network issues</p> <p>Solution: </p><pre><code>consumer = KafkaConsumer(\n    max_poll_interval_ms=600000,  # 10 minutes\n    session_timeout_ms=45000,\n    heartbeat_interval_ms=15000\n)\n</code></pre><p></p>"},{"location":"resources/faq/#q-out-of-disk-space","title":"Q: Out of disk space","text":"<p>Solutions:</p> <ol> <li> <p>Reduce retention: </p><pre><code>kafka-configs --alter --topic my-topic \\\n  --add-config retention.ms=86400000  # 1 day\n</code></pre><p></p> </li> <li> <p>Enable compression</p> </li> <li>Add more disks</li> <li>Use tiered storage (Kafka 3.6+)</li> </ol>"},{"location":"resources/faq/#security","title":"Security","text":""},{"location":"resources/faq/#q-how-to-enable-ssl","title":"Q: How to enable SSL?","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers='broker:9093',\n    security_protocol='SSL',\n    ssl_cafile='/path/to/ca-cert',\n    ssl_certfile='/path/to/client-cert',\n    ssl_keyfile='/path/to/client-key'\n)\n</code></pre> <p>Full SSL setup \u2192</p>"},{"location":"resources/faq/#q-whats-the-difference-between-ssl-and-sasl","title":"Q: What's the difference between SSL and SASL?","text":"<ul> <li>SSL (TLS): Encryption (secure data in transit)</li> <li>SASL: Authentication (verify identity)</li> </ul> <p>Best practice: Use both (<code>SASL_SSL</code>)</p>"},{"location":"resources/faq/#advanced","title":"Advanced","text":""},{"location":"resources/faq/#q-when-to-use-kafka-streams-vs-flink","title":"Q: When to use Kafka Streams vs Flink?","text":"Feature Kafka Streams Flink Setup Library (no cluster) Requires cluster Use Case Kafka-native pipelines Complex CEP Learning Curve Easy Steep <p>Use Kafka Streams for simple transformations/aggregations.</p> <p>Use Flink for complex multi-source processing.</p>"},{"location":"resources/faq/#q-can-i-change-partition-count-later","title":"Q: Can I change partition count later?","text":"<p>Yes, but: - Can only increase (not decrease) - Breaks keyed ordering (messages redistribute)</p> <pre><code>kafka-topics --alter --topic my-topic --partitions 10\n</code></pre> <p>\u26a0\ufe0f Warning: Existing keyed messages won't move partitions.</p>"},{"location":"resources/faq/#still-have-questions","title":"Still Have Questions?","text":"<p>Get Help</p> <ul> <li>Kafka Users Mailing List</li> <li>Stack Overflow</li> <li>Confluent Community Slack</li> </ul>"},{"location":"resources/glossary/","title":"Glossary","text":""},{"location":"resources/glossary/#glossary","title":"Glossary","text":""},{"location":"resources/glossary/#kafka-terminology","title":"\ud83d\udcd6 Kafka Terminology","text":"<p>Quick reference for Kafka concepts and terms.</p>"},{"location":"resources/glossary/#core-concepts","title":"Core Concepts","text":"Apache Kafka Distributed event streaming platform for high-throughput, fault-tolerant data pipelines. Event A record of something that happened (e.g., order placed, user clicked). Also called a message or record. Topic A category or feed name to which events are published. Like a database table or folder. Partition A subdivision of a topic that enables parallelism. Each partition is an ordered, immutable sequence of events. Offset A unique sequential ID assigned to each event within a partition. Used to track position. Broker A Kafka server that stores data and serves client requests. Cluster A group of Kafka brokers working together."},{"location":"resources/glossary/#producers-consumers","title":"Producers &amp; Consumers","text":"Producer An application that publishes (writes) events to Kafka topics. Consumer An application that subscribes to (reads) events from Kafka topics. Consumer Group A group of consumers sharing the same <code>group.id</code> that coordinate to consume a topic's partitions. Rebalancing Process where partitions are redistributed among consumers in a group (e.g., when a consumer joins/leaves)."},{"location":"resources/glossary/#replication-durability","title":"Replication &amp; Durability","text":"Replication Factor Number of copies of each partition. E.g., replication factor of 3 = 1 leader + 2 followers. Leader The broker replica that handles all reads and writes for a partition. Follower (Replica) A broker replica that replicates the leader and can become leader if needed. ISR (In-Sync Replicas) Set of replicas that are fully caught up with the leader. Acks (Acknowledgments) Producer setting controlling write durability: - <code>acks=0</code>: Fire and forget - <code>acks=1</code>: Leader acknowledges - <code>acks=all</code>: All ISRs acknowledge"},{"location":"resources/glossary/#processing-guarantees","title":"Processing Guarantees","text":"At-Least-Once Each message is processed at least once (may have duplicates on failure). At-Most-Once Each message is processed at most once (may lose messages on failure). Exactly-Once Each message is processed exactly once (no duplicates, no loss). Requires transactions. Idempotence Property where processing the same message multiple times has the same effect as once."},{"location":"resources/glossary/#advanced","title":"Advanced","text":"Kafka Streams Java library for building stream processing applications on Kafka. KStream Kafka Streams abstraction for an event stream (all events matter). KTable Kafka Streams abstraction for a changelog table (latest value per key). KSQL/ksqlDB SQL-like interface for stream processing on Kafka. Kafka Connect Framework for streaming data between Kafka and external systems (databases, S3, etc.). Source Connector Kafka Connect plugin that imports data from external system \u2192 Kafka. Sink Connector Kafka Connect plugin that exports data from Kafka \u2192 external system."},{"location":"resources/glossary/#schema-serialization","title":"Schema &amp; Serialization","text":"Schema Registry Central repository for managing event schemas with versioning. Avro Binary serialization format with schema evolution support. Protobuf Google's binary serialization format. Serializer Converts objects to bytes for sending to Kafka. Deserializer Converts bytes from Kafka back to objects."},{"location":"resources/glossary/#operations","title":"Operations","text":"ZooKeeper Coordination service used by Kafka (&lt; 3.3) for metadata management. Being replaced by KRaft. KRaft (Kafka Raft) New consensus protocol replacing ZooKeeper (Kafka 3.3+). Consumer Lag Difference between latest offset in partition and consumer's current offset. Retention How long Kafka keeps messages before deletion (time or size-based). Compaction Log cleanup policy that keeps only the latest value per key. Tiered Storage Feature to offload older data to cheaper storage (S3, Azure Blob)."},{"location":"resources/glossary/#patterns","title":"Patterns","text":"Event Sourcing Storing application state as a sequence of events. CQRS (Command Query Responsibility Segregation) Separate models for reading and writing data. CDC (Change Data Capture) Tracking database changes as events (e.g., using Debezium). Saga Pattern for distributed transactions across microservices. Dead Letter Queue (DLQ) Topic for storing messages that failed processing. Claim Check Pattern where large payloads are stored externally, and only reference is sent."},{"location":"resources/glossary/#performance","title":"Performance","text":"Batching Grouping multiple messages together for efficiency. Compression Reducing message size (snappy, gzip, lz4, zstd). Throughput Number of messages processed per unit of time. Latency Time delay from producer send to consumer receive. Back Pressure Mechanism to slow down producers when consumers can't keep up."},{"location":"resources/glossary/#security","title":"Security","text":"SSL/TLS Encryption protocol for securing data in transit. SASL (Simple Authentication and Security Layer) Framework for authentication (PLAIN, SCRAM, GSSAPI). ACL (Access Control List) Permissions defining who can read/write topics. mTLS (Mutual TLS) Both client and server authenticate each other. <p>Learn More</p> <p>Explore Cheat Sheets and Tools</p>"},{"location":"resources/tools/","title":"Tools & Links","text":""},{"location":"resources/tools/#tools-links","title":"Tools &amp; Links","text":""},{"location":"resources/tools/#essential-kafka-tools","title":"\ud83d\udee0\ufe0f Essential Kafka Tools","text":""},{"location":"resources/tools/#gui-tools","title":"\ud83d\udda5\ufe0f GUI Tools","text":""},{"location":"resources/tools/#kafka-ui-recommended","title":"Kafka UI (Recommended)","text":"<p>Free, open-source web UI for managing Kafka clusters.</p> <p>Features: - Browse topics and messages - Consumer group monitoring - Schema registry integration</p> <pre><code>docker run -p 8080:8080 \\\n  -e KAFKA_CLUSTERS_0_NAME=local \\\n  -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=localhost:9092 \\\n  provectuslabs/kafka-ui\n</code></pre> <p>Website \u2192</p>"},{"location":"resources/tools/#confluent-control-center","title":"Confluent Control Center","text":"<p>Enterprise-grade monitoring and management.</p> <p>Features: - Full cluster visibility - Stream monitoring - Data flow visualization</p> <p>Website \u2192</p>"},{"location":"resources/tools/#offset-explorer-kafka-tool","title":"Offset Explorer (Kafka Tool)","text":"<p>Desktop GUI for Kafka management.</p> <p>Download \u2192</p>"},{"location":"resources/tools/#monitoring","title":"\ud83d\udcca Monitoring","text":""},{"location":"resources/tools/#prometheus-grafana","title":"Prometheus + Grafana","text":"<pre><code># docker-compose.yml addition\nkafka-exporter:\n  image: danielqsj/kafka-exporter\n  command: --kafka.server=kafka:9092\n\nprometheus:\n  image: prom/prometheus\n  volumes:\n    - ./prometheus.yml:/etc/prometheus/prometheus.yml\n\ngrafana:\n  image: grafana/grafana\n  ports:\n    - \"3000:3000\"\n</code></pre> <p>Pre-built Dashboards: - Kafka Overview (ID: 7589) - Kafka Exporter (ID: 7589)</p>"},{"location":"resources/tools/#burrow","title":"Burrow","text":"<p>LinkedIn's consumer lag monitoring.</p> <p>GitHub \u2192</p>"},{"location":"resources/tools/#kafka-connect","title":"\ud83d\udd0c Kafka Connect","text":""},{"location":"resources/tools/#connector-hub","title":"Connector Hub","text":"<p>Browse 200+ connectors.</p> <p>Confluent Hub \u2192</p> <p>Popular Connectors: - JDBC Source/Sink - S3 Sink - Elasticsearch Sink - MongoDB Source/Sink - Debezium (CDC)</p>"},{"location":"resources/tools/#documentation","title":"\ud83d\udcda Documentation","text":"Resource Link Apache Kafka Docs kafka.apache.org Confluent Docs docs.confluent.io Kafka Improvement Proposals (KIPs) KIP List"},{"location":"resources/tools/#learning-resources","title":"\ud83c\udf93 Learning Resources","text":""},{"location":"resources/tools/#online-courses","title":"Online Courses","text":"<ul> <li>Kafka Fundamentals (Confluent)</li> <li>Kafka Streams (Udemy)</li> </ul>"},{"location":"resources/tools/#books","title":"Books","text":"<ul> <li>\"Kafka: The Definitive Guide\" by Neha Narkhede</li> <li>\"Kafka Streams in Action\" by William P. Bejeck Jr.</li> <li>\"Designing Event-Driven Systems\" by Ben Stopford</li> </ul>"},{"location":"resources/tools/#videos","title":"Videos","text":"<ul> <li>Kafka Summit Talks</li> <li>Confluent YouTube</li> </ul>"},{"location":"resources/tools/#client-libraries","title":"\ud83d\udcbb Client Libraries","text":"Language Library Link Java Apache Kafka Maven Python kafka-python PyPI Go Sarama GitHub Node.js KafkaJS npm C# Confluent.Kafka NuGet Rust rust-rdkafka crates.io"},{"location":"resources/tools/#managed-services","title":"\u2601\ufe0f Managed Services","text":"Provider Service Link Confluent Confluent Cloud confluent.cloud AWS Amazon MSK aws.amazon.com/msk Azure Azure Event Hubs azure.microsoft.com/event-hubs Google Cloud N/A Use self-managed on GKE Aiven Aiven for Kafka aiven.io/kafka"},{"location":"resources/tools/#docker-images","title":"\ud83d\udc33 Docker Images","text":"<pre><code># Official Kafka\ndocker pull apache/kafka:latest\n\n# Confluent Platform\ndocker pull confluentinc/cp-kafka:7.5.0\n\n# Bitnami (easier for development)\ndocker pull bitnami/kafka:latest\n</code></pre>"},{"location":"resources/tools/#community","title":"\ud83c\udf10 Community","text":"<ul> <li>Kafka Users Mailing List</li> <li>Stack Overflow [apache-kafka]</li> <li>Confluent Community Slack</li> <li>r/apachekafka</li> </ul> <p>Need Help?</p> <p>Check the FAQ or join the community!</p>"}]}